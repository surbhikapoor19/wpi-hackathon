{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watershed Preservation Opportunity Map Model\n",
    "\n",
    "## A GIS-based Decision Support System for Targeted Watershed Conservation\n",
    "\n",
    "This notebook builds a comprehensive model that overlays water quality data with land use, impervious cover, and environmental justice layers to identify where watershed preservation would have the greatest human benefit.\n",
    "\n",
    "### Key Innovation\n",
    "- Converts water quality chemistry into spatial intelligence\n",
    "- Prioritizes watershed preservation through quantifiable, local insights\n",
    "- Combines water data with EJ layers to identify high vulnerability neighborhoods\n",
    "- Creates a **Water Stress from Land Use Score** - a unified metric for conservation decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready. All dependencies loaded.\n",
      "GeoPandas version: 1.0.1\n",
      "Rasterio version: 1.4.3\n"
     ]
    }
   ],
   "source": [
    "# Core data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Geospatial analysis\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.ops import unary_union\n",
    "import pyproj\n",
    "from pyproj import Transformer\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium import plugins\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Environment ready. All dependencies loaded.\")\n",
    "print(f\"GeoPandas version: {gpd.__version__}\")\n",
    "print(f\"Rasterio version: {rasterio.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "### 2.1 Load Water Quality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (12596, 48)\n",
      "\n",
      "Columns: ['Unnamed: 0', 'Record_ID', 'Database_Version', 'Data_Year', 'Quality_Control_Status', 'Quality_Control_Date', 'Project_Name', 'Watershed_Name', 'Water_System_Code', 'Water_Body_Name', 'Station_ID', 'Station_Year_ID', 'Location_Description', 'Station_Type', 'River_Mile_Point', 'River_Mile_Point_Numeric', 'Latitude', 'Longitude', 'Field_Sheet_Code', 'Survey_Type', 'Sample_ID', 'Sample_Date', 'Sample_Date_Formatted', 'Sample_Time', 'Sample_Time_Formatted', 'Flow_Condition', 'Sample_Depth_m', 'Sample_Depth_Numeric', 'Depth_Qualifier_Code', 'Temperature_C', 'Temperature_C_Numeric', 'Temperature_Qualifier_Code', 'pH_Level', 'pH_Level_Numeric', 'pH_Qualifier_Code', 'Specific_Conductivity_uS_cm', 'Specific_Conductivity_Numeric', 'Conductivity_Qualifier_Code', 'Total_Dissolved_Solids_mg_L', 'Total_Dissolved_Solids_Numeric', 'TDS_Qualifier_Code', 'Dissolved_Oxygen_mg_L', 'Dissolved_Oxygen_Numeric', 'Dissolved_Oxygen_Qualifier_Code', 'Dissolved_Oxygen_Saturation_Percent', 'Dissolved_Oxygen_Saturation_Numeric', 'DO_Saturation_Qualifier_Code', 'Result_Comments']\n",
      "\n",
      "Data types:\n",
      "Unnamed: 0                               int64\n",
      "Record_ID                                int64\n",
      "Database_Version                        object\n",
      "Data_Year                                int64\n",
      "Quality_Control_Status                  object\n",
      "Quality_Control_Date                    object\n",
      "Project_Name                            object\n",
      "Watershed_Name                          object\n",
      "Water_System_Code                       object\n",
      "Water_Body_Name                         object\n",
      "Station_ID                              object\n",
      "Station_Year_ID                         object\n",
      "Location_Description                    object\n",
      "Station_Type                            object\n",
      "River_Mile_Point                        object\n",
      "River_Mile_Point_Numeric               float64\n",
      "Latitude                               float64\n",
      "Longitude                              float64\n",
      "Field_Sheet_Code                        object\n",
      "Survey_Type                             object\n",
      "Sample_ID                               object\n",
      "Sample_Date                             object\n",
      "Sample_Date_Formatted                   object\n",
      "Sample_Time                             object\n",
      "Sample_Time_Formatted                   object\n",
      "Flow_Condition                          object\n",
      "Sample_Depth_m                          object\n",
      "Sample_Depth_Numeric                   float64\n",
      "Depth_Qualifier_Code                    object\n",
      "Temperature_C                           object\n",
      "Temperature_C_Numeric                  float64\n",
      "Temperature_Qualifier_Code              object\n",
      "pH_Level                                object\n",
      "pH_Level_Numeric                       float64\n",
      "pH_Qualifier_Code                       object\n",
      "Specific_Conductivity_uS_cm             object\n",
      "Specific_Conductivity_Numeric          float64\n",
      "Conductivity_Qualifier_Code             object\n",
      "Total_Dissolved_Solids_mg_L             object\n",
      "Total_Dissolved_Solids_Numeric         float64\n",
      "TDS_Qualifier_Code                      object\n",
      "Dissolved_Oxygen_mg_L                   object\n",
      "Dissolved_Oxygen_Numeric               float64\n",
      "Dissolved_Oxygen_Qualifier_Code         object\n",
      "Dissolved_Oxygen_Saturation_Percent     object\n",
      "Dissolved_Oxygen_Saturation_Numeric    float64\n",
      "DO_Saturation_Qualifier_Code            object\n",
      "Result_Comments                         object\n",
      "dtype: object\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Record_ID</th>\n",
       "      <th>Database_Version</th>\n",
       "      <th>Data_Year</th>\n",
       "      <th>Quality_Control_Status</th>\n",
       "      <th>Quality_Control_Date</th>\n",
       "      <th>Project_Name</th>\n",
       "      <th>Watershed_Name</th>\n",
       "      <th>Water_System_Code</th>\n",
       "      <th>Water_Body_Name</th>\n",
       "      <th>Station_ID</th>\n",
       "      <th>Station_Year_ID</th>\n",
       "      <th>Location_Description</th>\n",
       "      <th>Station_Type</th>\n",
       "      <th>River_Mile_Point</th>\n",
       "      <th>River_Mile_Point_Numeric</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Field_Sheet_Code</th>\n",
       "      <th>Survey_Type</th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>Sample_Date</th>\n",
       "      <th>Sample_Date_Formatted</th>\n",
       "      <th>Sample_Time</th>\n",
       "      <th>Sample_Time_Formatted</th>\n",
       "      <th>Flow_Condition</th>\n",
       "      <th>Sample_Depth_m</th>\n",
       "      <th>Sample_Depth_Numeric</th>\n",
       "      <th>Depth_Qualifier_Code</th>\n",
       "      <th>Temperature_C</th>\n",
       "      <th>Temperature_C_Numeric</th>\n",
       "      <th>Temperature_Qualifier_Code</th>\n",
       "      <th>pH_Level</th>\n",
       "      <th>pH_Level_Numeric</th>\n",
       "      <th>pH_Qualifier_Code</th>\n",
       "      <th>Specific_Conductivity_uS_cm</th>\n",
       "      <th>Specific_Conductivity_Numeric</th>\n",
       "      <th>Conductivity_Qualifier_Code</th>\n",
       "      <th>Total_Dissolved_Solids_mg_L</th>\n",
       "      <th>Total_Dissolved_Solids_Numeric</th>\n",
       "      <th>TDS_Qualifier_Code</th>\n",
       "      <th>Dissolved_Oxygen_mg_L</th>\n",
       "      <th>Dissolved_Oxygen_Numeric</th>\n",
       "      <th>Dissolved_Oxygen_Qualifier_Code</th>\n",
       "      <th>Dissolved_Oxygen_Saturation_Percent</th>\n",
       "      <th>Dissolved_Oxygen_Saturation_Numeric</th>\n",
       "      <th>DO_Saturation_Qualifier_Code</th>\n",
       "      <th>Result_Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6767</td>\n",
       "      <td>112276</td>\n",
       "      <td>Extracted from WPP_WQData_2005-2020.mdb on 6/3...</td>\n",
       "      <td>2009</td>\n",
       "      <td>QC4</td>\n",
       "      <td>2011-12-15 13:05:31</td>\n",
       "      <td>Cape Cod (2009)</td>\n",
       "      <td>Cape Cod</td>\n",
       "      <td>9661650</td>\n",
       "      <td>Herring River</td>\n",
       "      <td>W1916</td>\n",
       "      <td>W1916</td>\n",
       "      <td>[Bound Brook Island Road, Wellfleet]</td>\n",
       "      <td>River/Stream</td>\n",
       "      <td>--</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.953519</td>\n",
       "      <td>-70.057176</td>\n",
       "      <td>09-D503-02</td>\n",
       "      <td>River</td>\n",
       "      <td>96-0465</td>\n",
       "      <td>9/2/2009</td>\n",
       "      <td>2009-09-02</td>\n",
       "      <td>11:26:06 AM</td>\n",
       "      <td>11:26:06</td>\n",
       "      <td>Flowing</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.1</td>\n",
       "      <td>14.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205</td>\n",
       "      <td>205.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6138</td>\n",
       "      <td>111647</td>\n",
       "      <td>Extracted from WPP_WQData_2005-2020.mdb on 6/3...</td>\n",
       "      <td>2009</td>\n",
       "      <td>QC4</td>\n",
       "      <td>2011-12-15 13:05:31</td>\n",
       "      <td>Narragansett-Mt. Hope Bay (2009)</td>\n",
       "      <td>Narragansett Bay (Shore)</td>\n",
       "      <td>5334025</td>\n",
       "      <td>Runnins River</td>\n",
       "      <td>W0651</td>\n",
       "      <td>Run1</td>\n",
       "      <td>[School Street, Seekonk]</td>\n",
       "      <td>River/Stream</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.100</td>\n",
       "      <td>41.788377</td>\n",
       "      <td>-71.329520</td>\n",
       "      <td>09-H509-08</td>\n",
       "      <td>River</td>\n",
       "      <td>53-0809</td>\n",
       "      <td>8/24/2009</td>\n",
       "      <td>2009-08-24</td>\n",
       "      <td>11:31:01 AM</td>\n",
       "      <td>11:31:01</td>\n",
       "      <td>Flowing</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>u</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>804</td>\n",
       "      <td>804.0</td>\n",
       "      <td>u</td>\n",
       "      <td>515</td>\n",
       "      <td>515.0</td>\n",
       "      <td>u</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11159</td>\n",
       "      <td>118461</td>\n",
       "      <td>Extracted from WPP_WQData_2005-2020.mdb on 6/3...</td>\n",
       "      <td>2006</td>\n",
       "      <td>QC4</td>\n",
       "      <td>2017-10-11 09:35:30</td>\n",
       "      <td>SMART: French &amp; Quinebaug (2006)</td>\n",
       "      <td>Quinebaug</td>\n",
       "      <td>4128875</td>\n",
       "      <td>Quinebaug River</td>\n",
       "      <td>W0601</td>\n",
       "      <td>QR00</td>\n",
       "      <td>[Holland Road bridge, Sturbridge]</td>\n",
       "      <td>River/Stream</td>\n",
       "      <td>15.202</td>\n",
       "      <td>15.202</td>\n",
       "      <td>42.109561</td>\n",
       "      <td>-72.118569</td>\n",
       "      <td>06-J004-01</td>\n",
       "      <td>River</td>\n",
       "      <td>SM-1803</td>\n",
       "      <td>7/19/2006</td>\n",
       "      <td>2006-07-19</td>\n",
       "      <td>8:53:03 AM</td>\n",
       "      <td>08:53:03</td>\n",
       "      <td>Flowing</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.4</td>\n",
       "      <td>27.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109</td>\n",
       "      <td>109.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71</td>\n",
       "      <td>71.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>92.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10384</td>\n",
       "      <td>117686</td>\n",
       "      <td>Extracted from WPP_WQData_2005-2020.mdb on 6/3...</td>\n",
       "      <td>2006</td>\n",
       "      <td>QC4</td>\n",
       "      <td>2017-10-11 09:35:30</td>\n",
       "      <td>Concord (2006)</td>\n",
       "      <td>Concord (SuAsCo)</td>\n",
       "      <td>8246775</td>\n",
       "      <td>Assabet River</td>\n",
       "      <td>W1473</td>\n",
       "      <td>AS08</td>\n",
       "      <td>[Robin Hill Street bridge, Marlborough (approx...</td>\n",
       "      <td>River/Stream</td>\n",
       "      <td>--</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.346501</td>\n",
       "      <td>-71.614603</td>\n",
       "      <td>06-A501-02</td>\n",
       "      <td>River</td>\n",
       "      <td>82-0323</td>\n",
       "      <td>6/5/2006</td>\n",
       "      <td>2006-06-05</td>\n",
       "      <td>9:00:31 AM</td>\n",
       "      <td>09:00:31</td>\n",
       "      <td>Flowing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i</td>\n",
       "      <td>15.1</td>\n",
       "      <td>15.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5513</td>\n",
       "      <td>126923</td>\n",
       "      <td>Extracted from WPP_WQData_2005-2020.mdb on 6/3...</td>\n",
       "      <td>2011</td>\n",
       "      <td>QC4</td>\n",
       "      <td>2019-06-19 10:28:42</td>\n",
       "      <td>SMART: Chicopee (2011)</td>\n",
       "      <td>Chicopee</td>\n",
       "      <td>3626500</td>\n",
       "      <td>Ware River</td>\n",
       "      <td>W0494</td>\n",
       "      <td>CBG</td>\n",
       "      <td>[south of Route 122 at weir downstream of Shaf...</td>\n",
       "      <td>River/Stream</td>\n",
       "      <td>30.29</td>\n",
       "      <td>30.290</td>\n",
       "      <td>42.391214</td>\n",
       "      <td>-72.064555</td>\n",
       "      <td>11-G004-02</td>\n",
       "      <td>River</td>\n",
       "      <td>SM-3709</td>\n",
       "      <td>8/30/2011</td>\n",
       "      <td>2011-08-30</td>\n",
       "      <td>9:45:31 AM</td>\n",
       "      <td>09:45:31</td>\n",
       "      <td>Flowing</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.4</td>\n",
       "      <td>18.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>54.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Record_ID                                   Database_Version  \\\n",
       "0        6767     112276  Extracted from WPP_WQData_2005-2020.mdb on 6/3...   \n",
       "1        6138     111647  Extracted from WPP_WQData_2005-2020.mdb on 6/3...   \n",
       "2       11159     118461  Extracted from WPP_WQData_2005-2020.mdb on 6/3...   \n",
       "3       10384     117686  Extracted from WPP_WQData_2005-2020.mdb on 6/3...   \n",
       "4        5513     126923  Extracted from WPP_WQData_2005-2020.mdb on 6/3...   \n",
       "\n",
       "   Data_Year Quality_Control_Status Quality_Control_Date  \\\n",
       "0       2009                    QC4  2011-12-15 13:05:31   \n",
       "1       2009                    QC4  2011-12-15 13:05:31   \n",
       "2       2006                    QC4  2017-10-11 09:35:30   \n",
       "3       2006                    QC4  2017-10-11 09:35:30   \n",
       "4       2011                    QC4  2019-06-19 10:28:42   \n",
       "\n",
       "                       Project_Name            Watershed_Name  \\\n",
       "0                   Cape Cod (2009)                  Cape Cod   \n",
       "1  Narragansett-Mt. Hope Bay (2009)  Narragansett Bay (Shore)   \n",
       "2  SMART: French & Quinebaug (2006)                 Quinebaug   \n",
       "3                    Concord (2006)          Concord (SuAsCo)   \n",
       "4            SMART: Chicopee (2011)                  Chicopee   \n",
       "\n",
       "  Water_System_Code  Water_Body_Name Station_ID Station_Year_ID  \\\n",
       "0           9661650    Herring River      W1916           W1916   \n",
       "1           5334025    Runnins River      W0651            Run1   \n",
       "2           4128875  Quinebaug River      W0601            QR00   \n",
       "3           8246775    Assabet River      W1473            AS08   \n",
       "4           3626500       Ware River      W0494             CBG   \n",
       "\n",
       "                                Location_Description  Station_Type  \\\n",
       "0               [Bound Brook Island Road, Wellfleet]  River/Stream   \n",
       "1                           [School Street, Seekonk]  River/Stream   \n",
       "2                  [Holland Road bridge, Sturbridge]  River/Stream   \n",
       "3  [Robin Hill Street bridge, Marlborough (approx...  River/Stream   \n",
       "4  [south of Route 122 at weir downstream of Shaf...  River/Stream   \n",
       "\n",
       "  River_Mile_Point  River_Mile_Point_Numeric   Latitude  Longitude  \\\n",
       "0               --                       NaN  41.953519 -70.057176   \n",
       "1              1.1                     1.100  41.788377 -71.329520   \n",
       "2           15.202                    15.202  42.109561 -72.118569   \n",
       "3               --                       NaN  42.346501 -71.614603   \n",
       "4            30.29                    30.290  42.391214 -72.064555   \n",
       "\n",
       "  Field_Sheet_Code Survey_Type Sample_ID Sample_Date Sample_Date_Formatted  \\\n",
       "0       09-D503-02       River   96-0465    9/2/2009            2009-09-02   \n",
       "1       09-H509-08       River   53-0809   8/24/2009            2009-08-24   \n",
       "2       06-J004-01       River   SM-1803   7/19/2006            2006-07-19   \n",
       "3       06-A501-02       River   82-0323    6/5/2006            2006-06-05   \n",
       "4       11-G004-02       River   SM-3709   8/30/2011            2011-08-30   \n",
       "\n",
       "   Sample_Time Sample_Time_Formatted Flow_Condition Sample_Depth_m  \\\n",
       "0  11:26:06 AM              11:26:06        Flowing            0.3   \n",
       "1  11:31:01 AM              11:31:01        Flowing            0.7   \n",
       "2   8:53:03 AM              08:53:03        Flowing            0.2   \n",
       "3   9:00:31 AM              09:00:31        Flowing            0.0   \n",
       "4   9:45:31 AM              09:45:31        Flowing            1.8   \n",
       "\n",
       "   Sample_Depth_Numeric Depth_Qualifier_Code Temperature_C  \\\n",
       "0                   0.3                  NaN          14.1   \n",
       "1                   0.7                  NaN          22.8   \n",
       "2                   0.2                  NaN          27.4   \n",
       "3                   0.0                    i          15.1   \n",
       "4                   1.8                  NaN          18.4   \n",
       "\n",
       "   Temperature_C_Numeric Temperature_Qualifier_Code pH_Level  \\\n",
       "0                   14.1                        NaN      5.3   \n",
       "1                   22.8                          u      6.5   \n",
       "2                   27.4                        NaN      7.0   \n",
       "3                   15.1                        NaN       --   \n",
       "4                   18.4                        NaN      5.7   \n",
       "\n",
       "   pH_Level_Numeric pH_Qualifier_Code Specific_Conductivity_uS_cm  \\\n",
       "0               5.3               NaN                         205   \n",
       "1               6.5               NaN                         804   \n",
       "2               7.0               NaN                         109   \n",
       "3               NaN               NaN                          --   \n",
       "4               5.7               NaN                          54   \n",
       "\n",
       "   Specific_Conductivity_Numeric Conductivity_Qualifier_Code  \\\n",
       "0                          205.0                         NaN   \n",
       "1                          804.0                           u   \n",
       "2                          109.0                         NaN   \n",
       "3                            NaN                         NaN   \n",
       "4                           54.0                         NaN   \n",
       "\n",
       "  Total_Dissolved_Solids_mg_L  Total_Dissolved_Solids_Numeric  \\\n",
       "0                         131                           131.0   \n",
       "1                         515                           515.0   \n",
       "2                          71                            71.0   \n",
       "3                          --                             NaN   \n",
       "4                          35                            35.0   \n",
       "\n",
       "  TDS_Qualifier_Code Dissolved_Oxygen_mg_L  Dissolved_Oxygen_Numeric  \\\n",
       "0                NaN                   4.4                       4.4   \n",
       "1                  u                   4.2                       4.2   \n",
       "2                NaN                   7.2                       7.2   \n",
       "3                NaN                   7.3                       7.3   \n",
       "4                NaN                   8.1                       8.1   \n",
       "\n",
       "  Dissolved_Oxygen_Qualifier_Code Dissolved_Oxygen_Saturation_Percent  \\\n",
       "0                             NaN                                  43   \n",
       "1                             NaN                                  50   \n",
       "2                             NaN                                  92   \n",
       "3                             NaN                                  74   \n",
       "4                             NaN                                  87   \n",
       "\n",
       "   Dissolved_Oxygen_Saturation_Numeric DO_Saturation_Qualifier_Code  \\\n",
       "0                                 43.0                          NaN   \n",
       "1                                 50.0                          NaN   \n",
       "2                                 92.0                          NaN   \n",
       "3                                 74.0                          NaN   \n",
       "4                                 87.0                          NaN   \n",
       "\n",
       "  Result_Comments  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load water quality data\n",
    "# Replace with your actual data path\n",
    "water_quality_df = pd.read_csv('sample_water_quality.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {water_quality_df.shape}\")\n",
    "print(f\"\\nColumns: {water_quality_df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(water_quality_df.dtypes)\n",
    "print(f\"\\nFirst few rows:\")\n",
    "water_quality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water quality data preprocessed successfully.\n",
      "Available parameters: ['pH', 'conductivity', 'temperature', 'TDS', 'DO']\n"
     ]
    }
   ],
   "source": [
    "# Map actual column names to simplified names for analysis\n",
    "column_mapping = {\n",
    "    'pH_Level_Numeric': 'pH',\n",
    "    'Specific_Conductivity_Numeric': 'conductivity',\n",
    "    'Temperature_C_Numeric': 'temperature',\n",
    "    'Total_Dissolved_Solids_Numeric': 'TDS',\n",
    "    'Dissolved_Oxygen_Numeric': 'DO'\n",
    "}\n",
    "\n",
    "# Create simplified column names\n",
    "for old_col, new_col in column_mapping.items():\n",
    "    if old_col in water_quality_df.columns:\n",
    "        water_quality_df[new_col] = pd.to_numeric(water_quality_df[old_col], errors='coerce')\n",
    "\n",
    "# Parse datetime if needed\n",
    "if 'Sample_Date_Formatted' in water_quality_df.columns:\n",
    "    water_quality_df['date'] = pd.to_datetime(water_quality_df['Sample_Date_Formatted'], errors='coerce')\n",
    "    water_quality_df['year'] = water_quality_df['date'].dt.year\n",
    "    water_quality_df['month'] = water_quality_df['date'].dt.month\n",
    "    water_quality_df['season'] = water_quality_df['date'].dt.month % 12 // 3 + 1\n",
    "elif 'Sample_Date' in water_quality_df.columns:\n",
    "    water_quality_df['date'] = pd.to_datetime(water_quality_df['Sample_Date'], errors='coerce')\n",
    "    water_quality_df['year'] = water_quality_df['date'].dt.year\n",
    "    water_quality_df['month'] = water_quality_df['date'].dt.month\n",
    "    water_quality_df['season'] = water_quality_df['date'].dt.month % 12 // 3 + 1\n",
    "\n",
    "# Key water quality parameters for analysis\n",
    "key_params = ['pH', 'conductivity', 'temperature', 'TDS', 'DO']\n",
    "\n",
    "# Ensure all key parameters are numeric (already done above, but double-check)\n",
    "for param in key_params:\n",
    "    if param in water_quality_df.columns:\n",
    "        water_quality_df[param] = pd.to_numeric(water_quality_df[param], errors='coerce')\n",
    "\n",
    "print(\"Water quality data preprocessed successfully.\")\n",
    "print(f\"Available parameters: {[p for p in key_params if p in water_quality_df.columns]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_quality_df.columns = water_quality_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Station-Level Aggregation and Stress Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated metrics for 1320 stations\n",
      "\n",
      "Station metrics columns: ['station_id', 'n_observations', 'latitude', 'longitude', 'pH_mean', 'pH_std', 'pH_cv', 'pH_p25', 'pH_median', 'pH_p75']...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>n_observations</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>pH_mean</th>\n",
       "      <th>pH_std</th>\n",
       "      <th>pH_cv</th>\n",
       "      <th>pH_p25</th>\n",
       "      <th>pH_median</th>\n",
       "      <th>pH_p75</th>\n",
       "      <th>pH_p95</th>\n",
       "      <th>pH_extreme_freq</th>\n",
       "      <th>conductivity_mean</th>\n",
       "      <th>conductivity_std</th>\n",
       "      <th>conductivity_cv</th>\n",
       "      <th>conductivity_p25</th>\n",
       "      <th>conductivity_median</th>\n",
       "      <th>conductivity_p75</th>\n",
       "      <th>conductivity_p95</th>\n",
       "      <th>conductivity_extreme_freq</th>\n",
       "      <th>temperature_mean</th>\n",
       "      <th>temperature_std</th>\n",
       "      <th>temperature_cv</th>\n",
       "      <th>temperature_p25</th>\n",
       "      <th>temperature_median</th>\n",
       "      <th>temperature_p75</th>\n",
       "      <th>temperature_p95</th>\n",
       "      <th>temperature_extreme_freq</th>\n",
       "      <th>TDS_mean</th>\n",
       "      <th>TDS_std</th>\n",
       "      <th>TDS_cv</th>\n",
       "      <th>TDS_p25</th>\n",
       "      <th>TDS_median</th>\n",
       "      <th>TDS_p75</th>\n",
       "      <th>TDS_p95</th>\n",
       "      <th>TDS_extreme_freq</th>\n",
       "      <th>DO_mean</th>\n",
       "      <th>DO_std</th>\n",
       "      <th>DO_cv</th>\n",
       "      <th>DO_p25</th>\n",
       "      <th>DO_median</th>\n",
       "      <th>DO_p75</th>\n",
       "      <th>DO_p95</th>\n",
       "      <th>DO_extreme_freq</th>\n",
       "      <th>pH_trend</th>\n",
       "      <th>pH_trend_pvalue</th>\n",
       "      <th>conductivity_trend</th>\n",
       "      <th>conductivity_trend_pvalue</th>\n",
       "      <th>temperature_trend</th>\n",
       "      <th>temperature_trend_pvalue</th>\n",
       "      <th>TDS_trend</th>\n",
       "      <th>TDS_trend_pvalue</th>\n",
       "      <th>DO_trend</th>\n",
       "      <th>DO_trend_pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W1916</td>\n",
       "      <td>8</td>\n",
       "      <td>41.953519</td>\n",
       "      <td>-70.057176</td>\n",
       "      <td>5.857143</td>\n",
       "      <td>0.386683</td>\n",
       "      <td>0.066019</td>\n",
       "      <td>5.600</td>\n",
       "      <td>5.90</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.27</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>204.857143</td>\n",
       "      <td>28.933421</td>\n",
       "      <td>0.141237</td>\n",
       "      <td>202.00</td>\n",
       "      <td>206.0</td>\n",
       "      <td>221.00</td>\n",
       "      <td>232.70</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>15.625000</td>\n",
       "      <td>2.081037</td>\n",
       "      <td>0.133186</td>\n",
       "      <td>14.1</td>\n",
       "      <td>15.25</td>\n",
       "      <td>15.85</td>\n",
       "      <td>19.020</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>18.681542</td>\n",
       "      <td>0.142607</td>\n",
       "      <td>129.00</td>\n",
       "      <td>132.0</td>\n",
       "      <td>141.50</td>\n",
       "      <td>148.90</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>5.425000</td>\n",
       "      <td>1.187735</td>\n",
       "      <td>0.218937</td>\n",
       "      <td>4.400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.500</td>\n",
       "      <td>6.995</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W0651</td>\n",
       "      <td>8</td>\n",
       "      <td>41.788377</td>\n",
       "      <td>-71.329520</td>\n",
       "      <td>6.737500</td>\n",
       "      <td>0.106066</td>\n",
       "      <td>0.015743</td>\n",
       "      <td>6.700</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>540.250000</td>\n",
       "      <td>200.335255</td>\n",
       "      <td>0.370820</td>\n",
       "      <td>416.75</td>\n",
       "      <td>465.5</td>\n",
       "      <td>621.75</td>\n",
       "      <td>856.00</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.730952</td>\n",
       "      <td>0.196366</td>\n",
       "      <td>16.2</td>\n",
       "      <td>18.50</td>\n",
       "      <td>22.20</td>\n",
       "      <td>23.580</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>345.750000</td>\n",
       "      <td>128.091206</td>\n",
       "      <td>0.370473</td>\n",
       "      <td>266.75</td>\n",
       "      <td>298.0</td>\n",
       "      <td>398.00</td>\n",
       "      <td>547.50</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>6.175000</td>\n",
       "      <td>1.398724</td>\n",
       "      <td>0.226514</td>\n",
       "      <td>5.375</td>\n",
       "      <td>6.2</td>\n",
       "      <td>7.175</td>\n",
       "      <td>7.895</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W0601</td>\n",
       "      <td>44</td>\n",
       "      <td>42.109561</td>\n",
       "      <td>-72.118569</td>\n",
       "      <td>6.590909</td>\n",
       "      <td>0.370304</td>\n",
       "      <td>0.056184</td>\n",
       "      <td>6.375</td>\n",
       "      <td>6.65</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>116.250000</td>\n",
       "      <td>18.263097</td>\n",
       "      <td>0.157102</td>\n",
       "      <td>108.50</td>\n",
       "      <td>117.5</td>\n",
       "      <td>125.25</td>\n",
       "      <td>145.55</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>12.747727</td>\n",
       "      <td>8.413955</td>\n",
       "      <td>0.660036</td>\n",
       "      <td>5.5</td>\n",
       "      <td>12.25</td>\n",
       "      <td>20.70</td>\n",
       "      <td>25.085</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>75.837209</td>\n",
       "      <td>11.912239</td>\n",
       "      <td>0.157076</td>\n",
       "      <td>72.50</td>\n",
       "      <td>77.0</td>\n",
       "      <td>82.50</td>\n",
       "      <td>94.80</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>10.561364</td>\n",
       "      <td>2.473226</td>\n",
       "      <td>0.234177</td>\n",
       "      <td>8.200</td>\n",
       "      <td>10.1</td>\n",
       "      <td>12.500</td>\n",
       "      <td>14.540</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.241240</td>\n",
       "      <td>-0.277590</td>\n",
       "      <td>0.204062</td>\n",
       "      <td>0.039049</td>\n",
       "      <td>0.700688</td>\n",
       "      <td>-0.222138</td>\n",
       "      <td>0.130712</td>\n",
       "      <td>-0.010987</td>\n",
       "      <td>0.712944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W1473</td>\n",
       "      <td>13</td>\n",
       "      <td>42.346501</td>\n",
       "      <td>-71.614603</td>\n",
       "      <td>7.020000</td>\n",
       "      <td>0.192354</td>\n",
       "      <td>0.027401</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.18</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>586.000000</td>\n",
       "      <td>134.188301</td>\n",
       "      <td>0.228990</td>\n",
       "      <td>643.00</td>\n",
       "      <td>645.0</td>\n",
       "      <td>646.00</td>\n",
       "      <td>649.20</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>18.676923</td>\n",
       "      <td>3.448467</td>\n",
       "      <td>0.184638</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.60</td>\n",
       "      <td>23.380</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>379.600000</td>\n",
       "      <td>86.436682</td>\n",
       "      <td>0.227705</td>\n",
       "      <td>416.00</td>\n",
       "      <td>418.0</td>\n",
       "      <td>419.00</td>\n",
       "      <td>419.80</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>7.538462</td>\n",
       "      <td>1.256674</td>\n",
       "      <td>0.166702</td>\n",
       "      <td>6.500</td>\n",
       "      <td>7.2</td>\n",
       "      <td>8.400</td>\n",
       "      <td>9.580</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166484</td>\n",
       "      <td>0.538479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.017033</td>\n",
       "      <td>0.864019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W0494</td>\n",
       "      <td>54</td>\n",
       "      <td>42.391214</td>\n",
       "      <td>-72.064555</td>\n",
       "      <td>6.273077</td>\n",
       "      <td>0.348732</td>\n",
       "      <td>0.055592</td>\n",
       "      <td>6.175</td>\n",
       "      <td>6.40</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.70</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>88.596154</td>\n",
       "      <td>15.375381</td>\n",
       "      <td>0.173545</td>\n",
       "      <td>78.00</td>\n",
       "      <td>87.0</td>\n",
       "      <td>101.25</td>\n",
       "      <td>113.90</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>12.864815</td>\n",
       "      <td>8.361506</td>\n",
       "      <td>0.649951</td>\n",
       "      <td>5.3</td>\n",
       "      <td>13.45</td>\n",
       "      <td>20.50</td>\n",
       "      <td>23.770</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>57.519231</td>\n",
       "      <td>9.820927</td>\n",
       "      <td>0.170742</td>\n",
       "      <td>50.75</td>\n",
       "      <td>57.0</td>\n",
       "      <td>65.25</td>\n",
       "      <td>72.45</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>10.598039</td>\n",
       "      <td>2.490983</td>\n",
       "      <td>0.235042</td>\n",
       "      <td>8.350</td>\n",
       "      <td>10.3</td>\n",
       "      <td>12.900</td>\n",
       "      <td>14.550</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.032622</td>\n",
       "      <td>-0.137668</td>\n",
       "      <td>0.337483</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.993471</td>\n",
       "      <td>-0.094980</td>\n",
       "      <td>0.299824</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.863500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id  n_observations   latitude  longitude   pH_mean    pH_std  \\\n",
       "0      W1916               8  41.953519 -70.057176  5.857143  0.386683   \n",
       "1      W0651               8  41.788377 -71.329520  6.737500  0.106066   \n",
       "2      W0601              44  42.109561 -72.118569  6.590909  0.370304   \n",
       "3      W1473              13  42.346501 -71.614603  7.020000  0.192354   \n",
       "4      W0494              54  42.391214 -72.064555  6.273077  0.348732   \n",
       "\n",
       "      pH_cv  pH_p25  pH_median  pH_p75  pH_p95  pH_extreme_freq  \\\n",
       "0  0.066019   5.600       5.90    6.15    6.27         0.285714   \n",
       "1  0.015743   6.700       6.80    6.80    6.80         0.125000   \n",
       "2  0.056184   6.375       6.65    6.90    7.00         0.136364   \n",
       "3  0.027401   7.000       7.10    7.10    7.18         0.400000   \n",
       "4  0.055592   6.175       6.40    6.50    6.70         0.211538   \n",
       "\n",
       "   conductivity_mean  conductivity_std  conductivity_cv  conductivity_p25  \\\n",
       "0         204.857143         28.933421         0.141237            202.00   \n",
       "1         540.250000        200.335255         0.370820            416.75   \n",
       "2         116.250000         18.263097         0.157102            108.50   \n",
       "3         586.000000        134.188301         0.228990            643.00   \n",
       "4          88.596154         15.375381         0.173545             78.00   \n",
       "\n",
       "   conductivity_median  conductivity_p75  conductivity_p95  \\\n",
       "0                206.0            221.00            232.70   \n",
       "1                465.5            621.75            856.00   \n",
       "2                117.5            125.25            145.55   \n",
       "3                645.0            646.00            649.20   \n",
       "4                 87.0            101.25            113.90   \n",
       "\n",
       "   conductivity_extreme_freq  temperature_mean  temperature_std  \\\n",
       "0                   0.285714         15.625000         2.081037   \n",
       "1                   0.250000         19.000000         3.730952   \n",
       "2                   0.227273         12.747727         8.413955   \n",
       "3                   0.400000         18.676923         3.448467   \n",
       "4                   0.211538         12.864815         8.361506   \n",
       "\n",
       "   temperature_cv  temperature_p25  temperature_median  temperature_p75  \\\n",
       "0        0.133186             14.1               15.25            15.85   \n",
       "1        0.196366             16.2               18.50            22.20   \n",
       "2        0.660036              5.5               12.25            20.70   \n",
       "3        0.184638             16.0               20.00            20.60   \n",
       "4        0.649951              5.3               13.45            20.50   \n",
       "\n",
       "   temperature_p95  temperature_extreme_freq    TDS_mean     TDS_std  \\\n",
       "0           19.020                  0.250000  131.000000   18.681542   \n",
       "1           23.580                  0.250000  345.750000  128.091206   \n",
       "2           25.085                  0.227273   75.837209   11.912239   \n",
       "3           23.380                  0.307692  379.600000   86.436682   \n",
       "4           23.770                  0.203704   57.519231    9.820927   \n",
       "\n",
       "     TDS_cv  TDS_p25  TDS_median  TDS_p75  TDS_p95  TDS_extreme_freq  \\\n",
       "0  0.142607   129.00       132.0   141.50   148.90          0.285714   \n",
       "1  0.370473   266.75       298.0   398.00   547.50          0.250000   \n",
       "2  0.157076    72.50        77.0    82.50    94.80          0.232558   \n",
       "3  0.227705   416.00       418.0   419.00   419.80          0.400000   \n",
       "4  0.170742    50.75        57.0    65.25    72.45          0.230769   \n",
       "\n",
       "     DO_mean    DO_std     DO_cv  DO_p25  DO_median  DO_p75  DO_p95  \\\n",
       "0   5.425000  1.187735  0.218937   4.400        5.0   6.500   6.995   \n",
       "1   6.175000  1.398724  0.226514   5.375        6.2   7.175   7.895   \n",
       "2  10.561364  2.473226  0.234177   8.200       10.1  12.500  14.540   \n",
       "3   7.538462  1.256674  0.166702   6.500        7.2   8.400   9.580   \n",
       "4  10.598039  2.490983  0.235042   8.350       10.3  12.900  14.550   \n",
       "\n",
       "   DO_extreme_freq  pH_trend  pH_trend_pvalue  conductivity_trend  \\\n",
       "0         0.250000       NaN              NaN                 NaN   \n",
       "1         0.250000       NaN              NaN                 NaN   \n",
       "2         0.204545  0.005201         0.241240           -0.277590   \n",
       "3         0.307692       NaN              NaN                 NaN   \n",
       "4         0.196078  0.006830         0.032622           -0.137668   \n",
       "\n",
       "   conductivity_trend_pvalue  temperature_trend  temperature_trend_pvalue  \\\n",
       "0                        NaN                NaN                       NaN   \n",
       "1                        NaN                NaN                       NaN   \n",
       "2                   0.204062           0.039049                  0.700688   \n",
       "3                        NaN           0.166484                  0.538479   \n",
       "4                   0.337483           0.000606                  0.993471   \n",
       "\n",
       "   TDS_trend  TDS_trend_pvalue  DO_trend  DO_trend_pvalue  \n",
       "0        NaN               NaN       NaN              NaN  \n",
       "1        NaN               NaN       NaN              NaN  \n",
       "2  -0.222138          0.130712 -0.010987         0.712944  \n",
       "3        NaN               NaN -0.017033         0.864019  \n",
       "4  -0.094980          0.299824  0.004136         0.863500  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_station_metrics(df, station_col='station_id', params=key_params):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive water stress metrics for each monitoring station\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Handle case-insensitive column names\n",
    "    station_col_lower = station_col.lower()\n",
    "    df_lower = df.copy()\n",
    "    df_lower.columns = df_lower.columns.str.lower()\n",
    "    \n",
    "    # Find the actual station column name\n",
    "    if station_col_lower in df_lower.columns:\n",
    "        station_col_actual = df_lower.columns[df_lower.columns == station_col_lower][0]\n",
    "    else:\n",
    "        # Try to find station_id or similar\n",
    "        station_col_actual = [col for col in df_lower.columns if 'station' in col.lower()][0] if any('station' in col.lower() for col in df_lower.columns) else station_col_lower\n",
    "    \n",
    "    for station in df_lower[station_col_actual].unique():\n",
    "        station_data = df_lower[df_lower[station_col_actual] == station]\n",
    "        \n",
    "        station_metrics = {\n",
    "            'station_id': station,\n",
    "            'n_observations': len(station_data),\n",
    "            'latitude': station_data['latitude'].iloc[0] if 'latitude' in station_data.columns else None,\n",
    "            'longitude': station_data['longitude'].iloc[0] if 'longitude' in station_data.columns else None,\n",
    "        }\n",
    "        \n",
    "        for param in params:\n",
    "            param_lower = param.lower()\n",
    "            if param_lower in station_data.columns:\n",
    "                param_data = station_data[param_lower].dropna()\n",
    "                \n",
    "                if len(param_data) > 0:\n",
    "                    # Basic statistics\n",
    "                    station_metrics[f'{param}_mean'] = param_data.mean()\n",
    "                    station_metrics[f'{param}_std'] = param_data.std()\n",
    "                    station_metrics[f'{param}_cv'] = param_data.std() / param_data.mean() if param_data.mean() != 0 else 0\n",
    "                    \n",
    "                    # Percentiles for anomaly detection\n",
    "                    station_metrics[f'{param}_p25'] = param_data.quantile(0.25)\n",
    "                    station_metrics[f'{param}_median'] = param_data.median()\n",
    "                    station_metrics[f'{param}_p75'] = param_data.quantile(0.75)\n",
    "                    station_metrics[f'{param}_p95'] = param_data.quantile(0.95)\n",
    "                    \n",
    "                    # Extreme value frequency\n",
    "                    upper_threshold = param_data.quantile(0.9)\n",
    "                    lower_threshold = param_data.quantile(0.1)\n",
    "                    station_metrics[f'{param}_extreme_freq'] = (\n",
    "                        ((param_data > upper_threshold) | (param_data < lower_threshold)).sum() / len(param_data)\n",
    "                    )\n",
    "                    \n",
    "                    # Trend analysis if temporal data available\n",
    "                    if 'date' in station_data.columns and len(param_data) > 10:\n",
    "                        # Sort by date for trend analysis\n",
    "                        station_data_sorted = station_data.sort_values('date')\n",
    "                        param_data_sorted = station_data_sorted[param_lower].dropna()\n",
    "                        if len(param_data_sorted) > 10:\n",
    "                            x = np.arange(len(param_data_sorted))\n",
    "                            slope, intercept, r_value, p_value, std_err = stats.linregress(x, param_data_sorted.values)\n",
    "                            station_metrics[f'{param}_trend'] = slope\n",
    "                            station_metrics[f'{param}_trend_pvalue'] = p_value\n",
    "        \n",
    "        metrics.append(station_metrics)\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Calculate station-level metrics\n",
    "station_metrics_df = calculate_station_metrics(water_quality_df)\n",
    "print(f\"Calculated metrics for {len(station_metrics_df)} stations\")\n",
    "print(f\"\\nStation metrics columns: {station_metrics_df.columns.tolist()[:10]}...\")\n",
    "station_metrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GIS Data Integration\n",
    "\n",
    "### 3.1 Convert Stations to GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created GeoDataFrame with 1320 monitoring stations\n",
      "CRS: EPSG:26986\n"
     ]
    }
   ],
   "source": [
    "# Create GeoDataFrame from station metrics\n",
    "geometry = [Point(xy) for xy in zip(station_metrics_df['longitude'], station_metrics_df['latitude'])]\n",
    "stations_gdf = gpd.GeoDataFrame(station_metrics_df, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "# Transform to Massachusetts State Plane (meters) for accurate distance calculations\n",
    "ma_crs = 'EPSG:26986'  # NAD83 / Massachusetts Mainland\n",
    "stations_gdf = stations_gdf.to_crs(ma_crs)\n",
    "\n",
    "print(f\"Created GeoDataFrame with {len(stations_gdf)} monitoring stations\")\n",
    "print(f\"CRS: {stations_gdf.crs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load and Process GIS Layers\n",
    "\n",
    "Note: These are placeholder functions. Replace with actual MassGIS data URLs/paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_impervious_layer(stations_gdf):\n",
    "    \"\"\"Create mock impervious surface data for demonstration\"\"\"\n",
    "    bounds = stations_gdf.total_bounds\n",
    "    # Create random polygons representing impervious areas\n",
    "    mock_data = []\n",
    "    np.random.seed(42)\n",
    "    for i in range(100):\n",
    "        x = np.random.uniform(bounds[0], bounds[2])\n",
    "        y = np.random.uniform(bounds[1], bounds[3])\n",
    "        size = np.random.uniform(100, 1000)\n",
    "        poly = Point(x, y).buffer(size)\n",
    "        mock_data.append({\n",
    "            'geometry': poly,\n",
    "            'imperv_pct': np.random.uniform(30, 95)\n",
    "        })\n",
    "    return gpd.GeoDataFrame(mock_data, crs=ma_crs)\n",
    "\n",
    "def create_mock_landuse_layer(stations_gdf):\n",
    "    \"\"\"Create mock land use data for demonstration\"\"\"\n",
    "    bounds = stations_gdf.total_bounds\n",
    "    landuse_types = ['Forest', 'Urban', 'Agriculture', 'Wetland', 'Industrial']\n",
    "    mock_data = []\n",
    "    np.random.seed(43)\n",
    "    for i in range(150):\n",
    "        x = np.random.uniform(bounds[0], bounds[2])\n",
    "        y = np.random.uniform(bounds[1], bounds[3])\n",
    "        size = np.random.uniform(500, 5000)\n",
    "        poly = Point(x, y).buffer(size)\n",
    "        mock_data.append({\n",
    "            'geometry': poly,\n",
    "            'landuse': np.random.choice(landuse_types),\n",
    "            'area_sq_m': poly.area\n",
    "        })\n",
    "    return gpd.GeoDataFrame(mock_data, crs=ma_crs)\n",
    "\n",
    "def create_mock_watershed_layer(stations_gdf):\n",
    "    \"\"\"Create mock watershed boundaries for demonstration\"\"\"\n",
    "    from scipy.spatial import Voronoi\n",
    "    points = np.column_stack([stations_gdf.geometry.x, stations_gdf.geometry.y])\n",
    "    # Create Voronoi polygons as mock watersheds\n",
    "    vor = Voronoi(points)\n",
    "    # Simplified: use station buffers as watersheds\n",
    "    watersheds = []\n",
    "    for idx, station in stations_gdf.iterrows():\n",
    "        watersheds.append({\n",
    "            'geometry': station.geometry.buffer(5000),  # 5km radius\n",
    "            'watershed_id': f\"WS_{idx}\",\n",
    "            'area_sq_km': np.pi * 25  # Approximate area\n",
    "        })\n",
    "    return gpd.GeoDataFrame(watersheds, crs=ma_crs)\n",
    "\n",
    "def create_mock_ej_layer(stations_gdf):\n",
    "    \"\"\"Create mock EJ communities data for demonstration\"\"\"\n",
    "    bounds = stations_gdf.total_bounds\n",
    "    mock_data = []\n",
    "    np.random.seed(44)\n",
    "    for i in range(30):\n",
    "        x = np.random.uniform(bounds[0], bounds[2])\n",
    "        y = np.random.uniform(bounds[1], bounds[3])\n",
    "        size = np.random.uniform(2000, 8000)\n",
    "        poly = Point(x, y).buffer(size)\n",
    "        mock_data.append({\n",
    "            'geometry': poly,\n",
    "            'ej_criteria': np.random.choice(['Income', 'Minority', 'Both']),\n",
    "            'population': np.random.randint(1000, 50000),\n",
    "            'vulnerability_score': np.random.uniform(0.3, 1.0)\n",
    "        })\n",
    "    return gpd.GeoDataFrame(mock_data, crs=ma_crs)\n",
    "\n",
    "def create_mock_protected_layer(stations_gdf):\n",
    "    \"\"\"Create mock protected lands data for demonstration\"\"\"\n",
    "    bounds = stations_gdf.total_bounds\n",
    "    mock_data = []\n",
    "    np.random.seed(45)\n",
    "    for i in range(40):\n",
    "        x = np.random.uniform(bounds[0], bounds[2])\n",
    "        y = np.random.uniform(bounds[1], bounds[3])\n",
    "        size = np.random.uniform(1000, 10000)\n",
    "        poly = Point(x, y).buffer(size)\n",
    "        mock_data.append({\n",
    "            'geometry': poly,\n",
    "            'protection_type': np.random.choice(['State Park', 'Conservation', 'Wildlife Refuge']),\n",
    "            'area_hectares': poly.area / 10000\n",
    "        })\n",
    "    return gpd.GeoDataFrame(mock_data, crs=ma_crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "def map_landuse_categories(shp_link_value):\n",
    "    \"\"\"\n",
    "    Map shp_link values to standardized land use categories\n",
    "    \"\"\"\n",
    "    if pd.isna(shp_link_value):\n",
    "        return 'Other'\n",
    "    \n",
    "    shp_str = str(shp_link_value).lower()\n",
    "    \n",
    "    # Forest categories\n",
    "    if any(x in shp_str for x in ['forest', 'wood', 'tree', 'deciduous', 'coniferous']):\n",
    "        return 'Forest'\n",
    "    # Urban categories\n",
    "    elif any(x in shp_str for x in ['urban', 'residential', 'commercial', 'developed', 'impervious', 'building']):\n",
    "        return 'Urban'\n",
    "    # Industrial\n",
    "    elif any(x in shp_str for x in ['industrial', 'manufacturing', 'warehouse']):\n",
    "        return 'Industrial'\n",
    "    # Wetland\n",
    "    elif any(x in shp_str for x in ['wetland', 'marsh', 'swamp', 'bog']):\n",
    "        return 'Wetland'\n",
    "    # Agriculture\n",
    "    elif any(x in shp_str for x in ['agriculture', 'agricultural', 'crop', 'farm', 'field', 'pasture']):\n",
    "        return 'Agriculture'\n",
    "    # Water\n",
    "    elif any(x in shp_str for x in ['water', 'open water', 'river', 'lake', 'pond']):\n",
    "        return 'Water'\n",
    "    # Recreation\n",
    "    elif any(x in shp_str for x in ['recreation', 'park', 'golf']):\n",
    "        return 'Recreation'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def fetch_mass_dor_economic_data():\n",
    "    \"\"\"\n",
    "    Fetch real economic data from Massachusetts DOR Municipal Databank\n",
    "    \n",
    "    Source: https://dls-gw.dor.state.ma.us/reports/rdPage.aspx?rdReport=Socioeconomic.MedHouseholdFamInc\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with municipality income data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from mass_dor_economic_data import fetch_dor_income_data, load_dor_income_data_from_file\n",
    "        import os\n",
    "        \n",
    "        # Try to load from file if available\n",
    "        dor_file = None\n",
    "        for f in ['ma_income_data.csv', 'ma_dor_income.csv', 'dor_socioeconomic.csv']:\n",
    "            if os.path.exists(f):\n",
    "                dor_file = f\n",
    "                break\n",
    "        \n",
    "        if dor_file:\n",
    "            print(f\"Loading DOR data from file: {dor_file}\")\n",
    "            income_data = load_dor_income_data_from_file(dor_file)\n",
    "        else:\n",
    "            print(\"Using built-in MA DOR income data (2019 estimates)\")\n",
    "            income_data = fetch_dor_income_data()\n",
    "        \n",
    "        return income_data\n",
    "    except ImportError:\n",
    "        print(\"Warning: mass_dor_economic_data module not found. Using synthetic data.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load DOR data: {e}. Using synthetic data.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_census_data_for_stations(stations_df, api_key=None, use_dor_data=True):\n",
    "    \"\"\"\n",
    "    Fetch economic data for monitoring stations using MA DOR data or Census API.\n",
    "    \n",
    "    Priority: MA DOR data (municipality-level) > Census API (tract-level) > Synthetic proxies\n",
    "    \n",
    "    Args:\n",
    "        stations_df: DataFrame with 'latitude', 'longitude', and optionally 'Location_Description'\n",
    "        api_key: Optional Census API key\n",
    "        use_dor_data: If True, try to use MA DOR municipal data first\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with economic data merged to stations\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import time\n",
    "    \n",
    "    stations_with_econ = stations_df.copy()\n",
    "    \n",
    "    # Priority 1: Try MA DOR municipal data (most accurate for MA)\n",
    "    if use_dor_data:\n",
    "        try:\n",
    "            from mass_dor_economic_data import join_economic_data_to_stations, fetch_dor_income_data\n",
    "            \n",
    "            print(\"Attempting to use MA DOR municipal income data...\")\n",
    "            income_data = fetch_dor_income_data()\n",
    "            \n",
    "            # Determine location columns\n",
    "            lat_col = None\n",
    "            lon_col = None\n",
    "            location_col = None\n",
    "            \n",
    "            for col in ['latitude', 'Latitude', 'lat']:\n",
    "                if col in stations_df.columns:\n",
    "                    lat_col = col\n",
    "                    break\n",
    "            \n",
    "            for col in ['longitude', 'Longitude', 'lon', 'lng']:\n",
    "                if col in stations_df.columns:\n",
    "                    lon_col = col\n",
    "                    break\n",
    "            \n",
    "            for col in ['Location_Description', 'location_description', 'location', 'Location']:\n",
    "                if col in stations_df.columns:\n",
    "                    location_col = col\n",
    "                    break\n",
    "            \n",
    "            if lat_col and lon_col:\n",
    "                stations_with_econ = join_economic_data_to_stations(\n",
    "                    stations_df,\n",
    "                    income_data,\n",
    "                    lat_col=lat_col,\n",
    "                    lon_col=lon_col,\n",
    "                    location_col=location_col\n",
    "                )\n",
    "                \n",
    "                # Check if we got real data\n",
    "                if stations_with_econ['median_household_income'].notna().sum() > 0:\n",
    "                    print(f\" Successfully joined MA DOR economic data for {stations_with_econ['median_household_income'].notna().sum()} stations\")\n",
    "                    return stations_with_econ\n",
    "                else:\n",
    "                    print(\"  DOR data join returned no matches, falling back to synthetic data\")\n",
    "            else:\n",
    "                print(\"  Missing coordinate columns, falling back to synthetic data\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not use DOR data: {e}. Falling back to synthetic data.\")\n",
    "    \n",
    "# Fallback: Synthetic proxies (original method)\n",
    "    print(\"Using synthetic economic proxies based on location...\")\n",
    "    \n",
    "    # Initialize stations_with_census BEFORE any conditionals\n",
    "    stations_with_census = stations_with_econ.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Find latitude/longitude columns (handle different case variations)\n",
    "    lat_col_fallback = None\n",
    "    lon_col_fallback = None\n",
    "    for col in stations_with_census.columns:\n",
    "        col_lower = col.lower()\n",
    "        if col_lower in ['latitude', 'lat']:\n",
    "            lat_col_fallback = col\n",
    "        if col_lower in ['longitude', 'lon', 'lng']:\n",
    "            lon_col_fallback = col\n",
    "    \n",
    "    if lat_col_fallback and lon_col_fallback:\n",
    "    print(\"Using synthetic economic proxies based on location...\")\n",
    "    \n",
    "    # Initialize stations_with_census from stations_with_econ\n",
    "    \n",
    "    # Find latitude/longitude columns (handle different case variations)\n",
    "    lat_col_fallback = None\n",
    "    lon_col_fallback = None\n",
    "    for col in stations_with_census.columns:\n",
    "        if col.lower() in ['latitude', 'lat']:\n",
    "            lat_col_fallback = col\n",
    "        if col.lower() in ['longitude', 'lon', 'lng']:\n",
    "            lon_col_fallback = col\n",
    "    \n",
    "    if lat_col_fallback and lon_col_fallback:\n",
    "        # Use lat/lon to create proxy economic indicators\n",
    "        # Urban areas (higher lat, around -71) tend to have different characteristics\n",
    "        # This is a simplified proxy - replace with actual Census API calls\n",
    "        \n",
    "        # Calculate distance from Boston (approximate urban center)\n",
    "        boston_lat, boston_lon = 42.3601, -71.0589\n",
    "        \n",
    "        stations_with_census['dist_from_boston_km'] = np.sqrt(\n",
    "            (stations_with_census[lat_col_fallback] - boston_lat)**2 + \n",
    "            (stations_with_census[lon_col_fallback] - boston_lon)**2\n",
    "        ) * 111  # Approximate km conversion\n",
    "        \n",
    "        # Proxy economic indicators (correlate with distance from urban center)\n",
    "        # Add realistic variation to create more diverse economic profiles\n",
    "        # These would be replaced with actual Census API data\n",
    "        \n",
    "        # Base income varies by distance, with added noise for realism\n",
    "        base_income = 45000 + 35000 * np.exp(-stations_with_census['dist_from_boston_km'] / 50)\n",
    "        income_noise = np.random.normal(0, 8000, len(stations_with_census))  # Add variation\n",
    "        stations_with_census['median_household_income'] = np.clip(\n",
    "            base_income + income_noise,\n",
    "            30000, 120000  # Realistic MA income range\n",
    "        )\n",
    "        \n",
    "        # Add mean income (typically higher than median due to skew)\n",
    "        stations_with_census['mean_household_income'] = stations_with_census['median_household_income'] * 1.15\n",
    "        \n",
    "        # Income inequality proxy (ratio of mean to median)\n",
    "        stations_with_census['income_inequality_ratio'] = (\n",
    "            stations_with_census['mean_household_income'] / stations_with_census['median_household_income']\n",
    "        )\n",
    "        \n",
    "        # Poverty rate with variation\n",
    "        base_poverty = 5 + 15 * np.exp(-stations_with_census['dist_from_boston_km'] / 40)\n",
    "        poverty_noise = np.random.normal(0, 3, len(stations_with_census))\n",
    "        stations_with_census['poverty_rate'] = np.clip(\n",
    "            base_poverty + poverty_noise,\n",
    "            2, 30  # Realistic poverty range\n",
    "        )\n",
    "        \n",
    "        # Population density with variation\n",
    "        base_density = 5000 * np.exp(-stations_with_census['dist_from_boston_km'] / 30)\n",
    "        density_noise = np.random.normal(0, base_density * 0.3, len(stations_with_census))\n",
    "        stations_with_census['population_density'] = np.clip(\n",
    "            base_density + density_noise,\n",
    "            50, 15000  # Realistic density range (rural to urban)\n",
    "        )\n",
    "        \n",
    "        # Education with variation\n",
    "        base_college = 30 + 25 * np.exp(-stations_with_census['dist_from_boston_km'] / 50)\n",
    "        college_noise = np.random.normal(0, 8, len(stations_with_census))\n",
    "        stations_with_census['percent_college_educated'] = np.clip(\n",
    "            base_college + college_noise,\n",
    "            15, 75  # Realistic education range\n",
    "        )\n",
    "        \n",
    "        # Additional economic indicators\n",
    "        # Unemployment rate (correlates with poverty)\n",
    "        stations_with_census['unemployment_rate'] = np.clip(\n",
    "            stations_with_census['poverty_rate'] * 0.6 + np.random.normal(2, 1.5, len(stations_with_census)),\n",
    "            1, 12\n",
    "        )\n",
    "        \n",
    "        # Median home value (correlates with income)\n",
    "        stations_with_census['median_home_value'] = (\n",
    "            stations_with_census['median_household_income'] * 3.5 + \n",
    "            np.random.normal(0, 50000, len(stations_with_census))\n",
    "        ).clip(150000, 800000)\n",
    "        \n",
    "        # Per capita income (household income / average household size)\n",
    "        avg_household_size = 2.3 + np.random.normal(0, 0.3, len(stations_with_census))\n",
    "        stations_with_census['per_capita_income'] = (\n",
    "            stations_with_census['median_household_income'] / avg_household_size\n",
    "        )\n",
    "        \n",
    "        # Economic diversity index (proxy for economic stability)\n",
    "        # Higher diversity = more stable economy\n",
    "        income_cv = stations_with_census['median_household_income'].std() / stations_with_census['median_household_income'].mean()\n",
    "        stations_with_census['economic_diversity_index'] = np.clip(\n",
    "            1 - (income_cv / 2), 0.3, 0.9\n",
    "        ) + np.random.normal(0, 0.1, len(stations_with_census))\n",
    "        \n",
    "        # Low-income households percentage\n",
    "        stations_with_census['percent_low_income'] = np.clip(\n",
    "            stations_with_census['poverty_rate'] * 1.5 + np.random.normal(0, 2, len(stations_with_census)),\n",
    "            5, 40\n",
    "        )\n",
    "        \n",
    "        # EJ proxy based on income and population density\n",
    "        stations_with_census['ej_vulnerability_proxy'] = (\n",
    "            (stations_with_census['poverty_rate'] / 25) * 0.4 +\n",
    "            (1 - stations_with_census['median_household_income'] / 80000) * 0.3 +\n",
    "            (stations_with_census['population_density'] / 5000) * 0.3\n",
    "        ).clip(0, 1)\n",
    "        \n",
    "        stations_with_census['in_ej_community'] = (\n",
    "            stations_with_census['ej_vulnerability_proxy'] > 0.5\n",
    "        ).astype(int)\n",
    "        \n",
    "        print(f\" Created comprehensive economic indicators for {len(stations_with_census)} stations\")\n",
    "        print(f\"  - Median income range: ${stations_with_census['median_household_income'].min():,.0f} - ${stations_with_census['median_household_income'].max():,.0f}\")\n",
    "        print(f\"  - Mean income range: ${stations_with_census['mean_household_income'].min():,.0f} - ${stations_with_census['mean_household_income'].max():,.0f}\")\n",
    "        print(f\"  - Poverty rate range: {stations_with_census['poverty_rate'].min():.1f}% - {stations_with_census['poverty_rate'].max():.1f}%\")\n",
    "        print(f\"  - Population density range: {stations_with_census['population_density'].min():,.0f} - {stations_with_census['population_density'].max():,.0f}/km\")\n",
    "        print(f\"  - College educated range: {stations_with_census['percent_college_educated'].min():.1f}% - {stations_with_census['percent_college_educated'].max():.1f}%\")\n",
    "        print(f\"  - Unemployment rate range: {stations_with_census['unemployment_rate'].min():.1f}% - {stations_with_census['unemployment_rate'].max():.1f}%\")\n",
    "        print(f\"  - Median home value range: ${stations_with_census['median_home_value'].min():,.0f} - ${stations_with_census['median_home_value'].max():,.0f}\")\n",
    "        print(f\"  - EJ communities identified: {stations_with_census['in_ej_community'].sum()}\")\n",
    "        print(f\"  - Total economic indicators: {len([c for c in stations_with_census.columns if any(x in c for x in ['income', 'poverty', 'density', 'college', 'unemployment', 'home', 'economic'])])}\")\n",
    "    \n",
    "    return stations_with_census\n",
    "\n",
    "\n",
    "def load_massgis_layers(stations_gdf, ma_crs=\"EPSG:26986\", use_census_data=True):\n",
    "    \"\"\"\n",
    "    Load GIS layers from folders OR use Census/economic data as alternative.\n",
    "    \n",
    "    If use_census_data=True, fetches readily available Census/economic data instead\n",
    "    of relying on sparse GIS layers.\n",
    "    \n",
    "    stations_gdf: GeoDataFrame of monitoring stations\n",
    "    ma_crs: CRS to convert layers to\n",
    "    use_census_data: If True, use Census API data instead of GIS layers\n",
    "    \"\"\"\n",
    "    layers = {}\n",
    "    \n",
    "    if use_census_data:\n",
    "        print(\"Using Census/economic data instead of GIS layers (more reliable)\")\n",
    "        # Convert stations to DataFrame for census function\n",
    "        stations_df = pd.DataFrame({\n",
    "            'station_id': stations_gdf.get('station_id', range(len(stations_gdf))),\n",
    "            'latitude': stations_gdf.geometry.y if hasattr(stations_gdf.geometry, 'y') else None,\n",
    "            'longitude': stations_gdf.geometry.x if hasattr(stations_gdf.geometry, 'x') else None\n",
    "        })\n",
    "        \n",
    "        # Try to get lat/lon from existing columns\n",
    "        if stations_df['latitude'].isna().all():\n",
    "            # Try to extract from geometry if it's in WGS84\n",
    "            try:\n",
    "                stations_wgs = stations_gdf.to_crs('EPSG:4326')\n",
    "                stations_df['latitude'] = stations_wgs.geometry.y\n",
    "                stations_df['longitude'] = stations_wgs.geometry.x\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if not stations_df['latitude'].isna().all():\n",
    "            stations_with_census = fetch_census_data_for_stations(stations_df, use_dor_data=True)\n",
    "            layers['census_data'] = stations_with_census\n",
    "            return layers\n",
    "        else:\n",
    "            print(\"Warning: Could not extract coordinates. Falling back to GIS layers.\")\n",
    "    \n",
    "    # Original GIS loading code (fallback)\n",
    "    # Mapping layer name -> folder\n",
    "    layer_folders = {\n",
    "        'ej_communities': 'ej2020',\n",
    "        'landuse': 'Land Cover Land Use (2016) Index',\n",
    "        'protected': 'openspace'\n",
    "    }\n",
    "\n",
    "    for layer_name, folder in layer_folders.items():\n",
    "        try:\n",
    "            # Find the .shp file in the folder\n",
    "            shp_files = [f for f in os.listdir(folder) if f.endswith('.shp')]\n",
    "            if not shp_files:\n",
    "                raise FileNotFoundError(f\"No .shp file found in {folder}\")\n",
    "            \n",
    "            shp_path = os.path.join(folder, shp_files[0])\n",
    "            gdf = gpd.read_file(shp_path).to_crs(ma_crs)\n",
    "            \n",
    "            # Process landuse layer - map shp_link to categories\n",
    "            if layer_name == 'landuse' and 'shp_link' in gdf.columns:\n",
    "                gdf['landuse'] = gdf['shp_link'].apply(map_landuse_categories)\n",
    "                print(f\" Mapped {gdf['landuse'].nunique()} land use categories\")\n",
    "            \n",
    "            # Process EJ layer - create vulnerability score if missing\n",
    "            if layer_name == 'ej_communities':\n",
    "                if 'vulnerability_score' not in gdf.columns:\n",
    "                    # Create a simple vulnerability score based on available columns\n",
    "                    # Check for common EJ indicator columns\n",
    "                    if 'MINORITY' in gdf.columns or 'minority' in gdf.columns:\n",
    "                        minority_col = 'MINORITY' if 'MINORITY' in gdf.columns else 'minority'\n",
    "                        income_col = None\n",
    "                        if 'LOW_INCOME' in gdf.columns:\n",
    "                            income_col = 'LOW_INCOME'\n",
    "                        elif 'low_income' in gdf.columns:\n",
    "                            income_col = 'low_income'\n",
    "                        \n",
    "                        # Create composite vulnerability score\n",
    "                        gdf['vulnerability_score'] = 0.5\n",
    "                        if minority_col in gdf.columns:\n",
    "                            gdf['vulnerability_score'] += 0.3 * (gdf[minority_col] > 0).astype(int)\n",
    "                        if income_col and income_col in gdf.columns:\n",
    "                            gdf['vulnerability_score'] += 0.2 * (gdf[income_col] > 0).astype(int)\n",
    "                        gdf['vulnerability_score'] = gdf['vulnerability_score'].clip(0, 1)\n",
    "                    else:\n",
    "                        # Default vulnerability score\n",
    "                        gdf['vulnerability_score'] = 0.5\n",
    "            \n",
    "            layers[layer_name] = gdf\n",
    "            print(f\" Loaded {layer_name} layer from {shp_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Could not load {layer_name} layer: {e}\")\n",
    "            # Fallback to mock data\n",
    "            if layer_name == 'ej_communities':\n",
    "                layers[layer_name] = create_mock_ej_layer(stations_gdf)\n",
    "            elif layer_name == 'landuse':\n",
    "                layers[layer_name] = create_mock_landuse_layer(stations_gdf)\n",
    "            elif layer_name == 'protected':\n",
    "                layers[layer_name] = create_mock_protected_layer(stations_gdf)\n",
    "    \n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Census/economic data instead of GIS layers (more reliable)\n",
      "Attempting to use MA DOR municipal income data...\n",
      "Fetching MA DOR income data for year 2019...\n",
      "Loaded income data for 26 municipalities\n",
      "  DOR data join returned no matches, falling back to synthetic data\n",
      "Using synthetic economic proxies based on location...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stations_with_census' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gis_layers \u001b[38;5;241m=\u001b[39m load_massgis_layers(stations_gdf, use_census_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(gis_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data layers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcensus_data\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m gis_layers:\n",
      "Cell \u001b[0;32mIn[14], line 290\u001b[0m, in \u001b[0;36mload_massgis_layers\u001b[0;34m(stations_gdf, ma_crs, use_census_data)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stations_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 290\u001b[0m     stations_with_census \u001b[38;5;241m=\u001b[39m fetch_census_data_for_stations(stations_df, use_dor_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    291\u001b[0m     layers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcensus_data\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stations_with_census\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layers\n",
      "Cell \u001b[0;32mIn[14], line 152\u001b[0m, in \u001b[0;36mfetch_census_data_for_stations\u001b[0;34m(stations_df, api_key, use_dor_data)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stations_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stations_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# Use lat/lon to create proxy economic indicators\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Urban areas (higher lat, around -71) tend to have different characteristics\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# This is a simplified proxy - replace with actual Census API calls\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# Calculate distance from Boston (approximate urban center)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     boston_lat, boston_lon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42.3601\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m71.0589\u001b[39m\n\u001b[1;32m    151\u001b[0m     stations_with_census[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdist_from_boston_km\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\n\u001b[0;32m--> 152\u001b[0m         (stations_with_census[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m boston_lat)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m    153\u001b[0m         (stations_with_census[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m boston_lon)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    154\u001b[0m     ) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m111\u001b[39m  \u001b[38;5;66;03m# Approximate km conversion\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Proxy economic indicators (correlate with distance from urban center)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Add realistic variation to create more diverse economic profiles\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# These would be replaced with actual Census API data\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Base income varies by distance, with added noise for realism\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     base_income \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m45000\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m35000\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mstations_with_census[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdist_from_boston_km\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stations_with_census' is not defined"
     ]
    }
   ],
   "source": [
    "gis_layers = load_massgis_layers(stations_gdf, use_census_data=True)\n",
    "print(f\"\\nLoaded {len(gis_layers)} data layers\")\n",
    "if 'census_data' in gis_layers:\n",
    "    print(\" Using Census/economic data (more reliable than sparse GIS layers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_layers.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Analysis and Feature Engineering\n",
    "\n",
    "### 4.1 Calculate Land Characteristics Around Each Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_buffer_statistics(stations, layers, buffer_distances=[500, 1000, 2000, 5000], use_census_data=True):\n",
    "    \"\"\"\n",
    "    Calculate spatial statistics for stations using either GIS layers OR Census/economic data.\n",
    "    \n",
    "    If use_census_data=True and census_data is in layers, uses economic indicators instead\n",
    "    of land use percentages (which may be sparse/zero).\n",
    "    \"\"\"\n",
    "    stations_with_features = stations.copy()\n",
    "    \n",
    "    # Check if we're using census data instead of GIS\n",
    "    if use_census_data and 'census_data' in layers:\n",
    "        print(\"Using Census/economic data for spatial features (more reliable than sparse GIS)\")\n",
    "        census_df = layers['census_data']\n",
    "        \n",
    "        # Merge census data to stations\n",
    "        if 'station_id' in stations_with_features.columns and 'station_id' in census_df.columns:\n",
    "            stations_with_features = stations_with_features.merge(\n",
    "                census_df.drop(columns=['latitude', 'longitude'], errors='ignore'),\n",
    "                on='station_id',\n",
    "                how='left'\n",
    "            )\n",
    "        else:\n",
    "            # Merge by index if no station_id\n",
    "            for col in census_df.columns:\n",
    "                if col not in ['station_id', 'latitude', 'longitude']:\n",
    "                    stations_with_features[col] = census_df[col].values\n",
    "        \n",
    "        # Create derived features from economic data\n",
    "        # These serve as proxies for land use patterns\n",
    "        \n",
    "        # Urbanization proxy (higher income + density = more urban)\n",
    "        if 'median_household_income' in stations_with_features.columns:\n",
    "            stations_with_features['urbanization_proxy'] = (\n",
    "                (stations_with_features['median_household_income'] / 80000) * 0.5 +\n",
    "                (stations_with_features.get('population_density', 0) / 5000) * 0.5\n",
    "            ).clip(0, 1)\n",
    "        \n",
    "        # Natural cover proxy (inverse of urbanization, higher in rural areas)\n",
    "        if 'dist_from_boston_km' in stations_with_features.columns:\n",
    "            stations_with_features['natural_cover_proxy'] = (\n",
    "                1 - np.exp(-stations_with_features['dist_from_boston_km'] / 50)\n",
    "            ).clip(0, 1)\n",
    "        \n",
    "        # EJ vulnerability (already calculated in census function)\n",
    "        if 'ej_vulnerability_proxy' in stations_with_features.columns:\n",
    "            stations_with_features['ej_vulnerability'] = stations_with_features['ej_vulnerability_proxy']\n",
    "        \n",
    "        print(f\" Added {len([c for c in stations_with_features.columns if 'proxy' in c or 'income' in c or 'poverty' in c])} economic/spatial proxy features\")\n",
    "        return stations_with_features\n",
    "    \n",
    "    # Original GIS-based buffer statistics (fallback)\n",
    "    # Get CRS from stations\n",
    "    ma_crs = stations.crs\n",
    "    \n",
    "    for distance in buffer_distances:\n",
    "        print(f\"\\nCalculating statistics for {distance}m buffer...\")\n",
    "        \n",
    "        # Create buffers around stations\n",
    "        stations_buffered = stations.copy()\n",
    "        stations_buffered['buffer_geom'] = stations_buffered.geometry.buffer(distance)\n",
    "        \n",
    "        # Calculate land use composition\n",
    "        if 'landuse' in layers:\n",
    "            # Create buffer GeoDataFrame with proper geometry\n",
    "            buffer_gdf = gpd.GeoDataFrame(\n",
    "                stations_buffered[['station_id']],\n",
    "                geometry=stations_buffered['buffer_geom'],\n",
    "                crs=ma_crs\n",
    "            )\n",
    "            \n",
    "            # Use overlay for geometric intersection and clipping\n",
    "            try:\n",
    "                landuse_intersect = gpd.overlay(\n",
    "                    buffer_gdf,\n",
    "                    layers['landuse'][['geometry', 'landuse']], \n",
    "                    how='intersection',\n",
    "                    keep_geom_type=False \n",
    "                )\n",
    "                \n",
    "                # Calculate the area of the clipped geometry\n",
    "                landuse_intersect['intersect_area_sq_m'] = landuse_intersect.geometry.area\n",
    "                \n",
    "                # Calculate land use percentages for each category\n",
    "                for landuse_type in ['Forest', 'Urban', 'Industrial', 'Wetland', 'Agriculture']:\n",
    "                    # Filter by landuse type and group by station_id\n",
    "                    landuse_filtered = landuse_intersect[landuse_intersect['landuse'] == landuse_type]\n",
    "                    \n",
    "                    if len(landuse_filtered) > 0:\n",
    "                        landuse_area = (\n",
    "                            landuse_filtered\n",
    "                            .groupby('station_id')['intersect_area_sq_m']\n",
    "                            .sum()\n",
    "                        )\n",
    "                        \n",
    "                        # Calculate the theoretical total area of a perfect circle buffer\n",
    "                        total_area = np.pi * distance ** 2\n",
    "                        \n",
    "                        # Calculate percentage\n",
    "                        landuse_pct = (landuse_area / total_area * 100).reset_index()\n",
    "                        landuse_pct.columns = ['station_id', f'{landuse_type.lower()}_{distance}m_pct']\n",
    "                        stations_with_features = stations_with_features.merge(landuse_pct, on='station_id', how='left')\n",
    "                    else:\n",
    "                        # No data for this landuse type - set to 0\n",
    "                        stations_with_features[f'{landuse_type.lower()}_{distance}m_pct'] = 0\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error calculating landuse for {distance}m buffer: {e}\")\n",
    "                # Set default values\n",
    "                for landuse_type in ['Forest', 'Urban', 'Industrial', 'Wetland', 'Agriculture']:\n",
    "                    stations_with_features[f'{landuse_type.lower()}_{distance}m_pct'] = 0\n",
    "        \n",
    "        # Calculate impervious surface percentage (if available)\n",
    "        if 'impervious' in layers:\n",
    "            # Similar approach for impervious surfaces\n",
    "            buffer_gdf = gpd.GeoDataFrame(\n",
    "                stations_buffered[['station_id']],\n",
    "                geometry=stations_buffered['buffer_geom'],\n",
    "                crs=ma_crs\n",
    "            )\n",
    "            try:\n",
    "                imperv_intersect = gpd.overlay(\n",
    "                    buffer_gdf,\n",
    "                    layers['impervious'][['geometry', 'imperv_pct']],\n",
    "                    how='intersection',\n",
    "                    keep_geom_type=False\n",
    "                )\n",
    "                imperv_intersect['intersect_area_sq_m'] = imperv_intersect.geometry.area\n",
    "                total_area = np.pi * distance ** 2\n",
    "                \n",
    "                # Weighted average impervious percentage\n",
    "                imperv_weighted = (\n",
    "                    imperv_intersect.groupby('station_id')\n",
    "                    .apply(lambda x: (x['intersect_area_sq_m'] * x['imperv_pct']).sum() / x['intersect_area_sq_m'].sum())\n",
    "                    .reset_index()\n",
    "                )\n",
    "                imperv_weighted.columns = ['station_id', f'imperv_mean_{distance}m']\n",
    "                stations_with_features = stations_with_features.merge(imperv_weighted, on='station_id', how='left')\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error calculating impervious for {distance}m buffer: {e}\")\n",
    "        \n",
    "        # Check if within EJ community (only need to do once, not per buffer)\n",
    "        if 'ej_communities' in layers and distance == buffer_distances[0]:\n",
    "            stations_points = stations[['station_id', 'geometry']].copy()\n",
    "            try:\n",
    "                ej_joined = gpd.sjoin(stations_points, layers['ej_communities'], predicate='within', how='left')\n",
    "                \n",
    "                if 'vulnerability_score' in ej_joined.columns:\n",
    "                    ej_flags = ej_joined.groupby('station_id')['vulnerability_score'].max().reset_index()\n",
    "                    ej_flags.columns = ['station_id', 'ej_vulnerability']\n",
    "                    ej_flags['in_ej_community'] = (ej_flags['ej_vulnerability'] > 0).astype(int)\n",
    "                else:\n",
    "                    # If no vulnerability_score, just mark if in EJ community\n",
    "                    ej_flags = ej_joined.groupby('station_id').size().reset_index()\n",
    "                    ej_flags.columns = ['station_id', 'ej_count']\n",
    "                    ej_flags['in_ej_community'] = (ej_flags['ej_count'] > 0).astype(int)\n",
    "                    ej_flags['ej_vulnerability'] = ej_flags['in_ej_community'] * 0.5\n",
    "                    ej_flags = ej_flags[['station_id', 'ej_vulnerability', 'in_ej_community']]\n",
    "                \n",
    "                stations_with_features = stations_with_features.merge(ej_flags, on='station_id', how='left')\n",
    "                stations_with_features['in_ej_community'] = stations_with_features['in_ej_community'].fillna(0)\n",
    "                stations_with_features['ej_vulnerability'] = stations_with_features['ej_vulnerability'].fillna(0)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error calculating EJ communities: {e}\")\n",
    "                stations_with_features['in_ej_community'] = 0\n",
    "                stations_with_features['ej_vulnerability'] = 0\n",
    "    \n",
    "    # Fill NaN values with 0 for land use percentages\n",
    "    landuse_cols = [col for col in stations_with_features.columns if '_pct' in col or 'imperv_' in col]\n",
    "    for col in landuse_cols:\n",
    "        if col in stations_with_features.columns:\n",
    "            stations_with_features[col] = stations_with_features[col].fillna(0)\n",
    "    \n",
    "    return stations_with_features\n",
    "\n",
    "# Calculate spatial features\n",
    "stations_with_spatial = calculate_buffer_statistics(stations_gdf, gis_layers, use_census_data=True)\n",
    "print(f\"\\nTotal features after spatial analysis: {len(stations_with_spatial.columns)}\")\n",
    "economic_features = [col for col in stations_with_spatial.columns if any(x in col for x in ['income', 'poverty', 'density', 'proxy', 'ej'])]\n",
    "if economic_features:\n",
    "    print(f\"Sample economic/spatial features: {economic_features[:5]}\")\n",
    "else:\n",
    "    gis_features = [col for col in stations_with_spatial.columns if '500m' in col]\n",
    "    print(f\"Sample GIS features: {gis_features[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Water Stress from Land Use Score (WSLUS) Calculation\n",
    "\n",
    "### 5.1 Define the Multi-Factor Risk Scoring System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaterStressScorer:\n",
    "    \"\"\"\n",
    "    Calculate Water Stress from Land Use Score (WSLUS)\n",
    "    A unified metric combining water quality indicators with spatial land characteristics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, weights=None):\n",
    "        # Default weights for different stress factors\n",
    "        self.weights = weights or {\n",
    "            'conductivity_stress': 0.25,\n",
    "            'tds_stress': 0.20,\n",
    "            'ph_stress': 0.15,\n",
    "            'temperature_stress': 0.15,\n",
    "            'impervious_stress': 0.15,\n",
    "            'landuse_stress': 0.10\n",
    "        }\n",
    "        \n",
    "        # Thresholds for water quality parameters (based on EPA guidelines)\n",
    "        self.thresholds = {\n",
    "            'conductivity': {'low': 150, 'moderate': 500, 'high': 1500},  # S/cm\n",
    "            'tds': {'low': 100, 'moderate': 300, 'high': 500},  # mg/L\n",
    "            'ph': {'low': 6.5, 'optimal': 7.5, 'high': 8.5},\n",
    "            'temperature': {'low': 10, 'moderate': 20, 'high': 25},  # C\n",
    "            'do': {'critical': 4, 'low': 6, 'good': 8}  # mg/L\n",
    "        }\n",
    "    \n",
    "    def _get_column(self, df, possible_names):\n",
    "        \"\"\"Helper to find column with case-insensitive matching\"\"\"\n",
    "        for name in possible_names:\n",
    "            # Exact match\n",
    "            if name in df.columns:\n",
    "                return df[name]\n",
    "            # Case-insensitive match\n",
    "            matches = [col for col in df.columns if col.lower() == name.lower()]\n",
    "            if matches:\n",
    "                return df[matches[0]]\n",
    "        return None\n",
    "    \n",
    "    def calculate_conductivity_stress(self, df):\n",
    "        \"\"\"Calculate stress from conductivity (indicator of salt/runoff)\"\"\"\n",
    "        cond = self._get_column(df, ['conductivity_mean', 'conductivity'])\n",
    "        if cond is None:\n",
    "            return pd.Series(0, index=df.index)\n",
    "        stress = pd.Series(0.0, index=df.index)\n",
    "        \n",
    "        # Low stress\n",
    "        stress[cond < self.thresholds['conductivity']['low']] = 0.2\n",
    "        # Moderate stress\n",
    "        stress[(cond >= self.thresholds['conductivity']['low']) & \n",
    "               (cond < self.thresholds['conductivity']['moderate'])] = 0.5\n",
    "        # High stress\n",
    "        stress[(cond >= self.thresholds['conductivity']['moderate']) & \n",
    "               (cond < self.thresholds['conductivity']['high'])] = 0.8\n",
    "        # Severe stress\n",
    "        stress[cond >= self.thresholds['conductivity']['high']] = 1.0\n",
    "        \n",
    "        # Amplify by variability (high CV indicates instability)\n",
    "        cond_cv = self._get_column(df, ['conductivity_cv', 'conductivity_std'])\n",
    "        if cond_cv is not None:\n",
    "            cond_mean = self._get_column(df, ['conductivity_mean', 'conductivity'])\n",
    "            if cond_mean is not None and cond_mean.mean() > 0:\n",
    "                cv = cond_cv.fillna(0) / cond_mean.fillna(1)\n",
    "                stress = stress * (1 + cv * 0.3)\n",
    "        \n",
    "        return stress.clip(0, 1)\n",
    "    \n",
    "    def calculate_tds_stress(self, df):\n",
    "        \"\"\"Calculate stress from Total Dissolved Solids\"\"\"\n",
    "        tds = self._get_column(df, ['TDS_mean', 'tds_mean', 'TDS', 'tds'])\n",
    "        if tds is None:\n",
    "            return pd.Series(0, index=df.index)\n",
    "        stress = pd.Series(0.0, index=df.index)\n",
    "        \n",
    "        stress[tds < self.thresholds['tds']['low']] = 0.2\n",
    "        stress[(tds >= self.thresholds['tds']['low']) & \n",
    "               (tds < self.thresholds['tds']['moderate'])] = 0.5\n",
    "        stress[(tds >= self.thresholds['tds']['moderate']) & \n",
    "               (tds < self.thresholds['tds']['high'])] = 0.8\n",
    "        stress[tds >= self.thresholds['tds']['high']] = 1.0\n",
    "        \n",
    "        return stress.clip(0, 1)\n",
    "    \n",
    "    def calculate_ph_stress(self, df):\n",
    "        \"\"\"Calculate stress from pH deviation from optimal range\"\"\"\n",
    "        ph = self._get_column(df, ['pH_mean', 'ph_mean', 'pH', 'ph'])\n",
    "        if ph is None:\n",
    "            return pd.Series(0, index=df.index)\n",
    "        optimal = self.thresholds['ph']['optimal']\n",
    "        \n",
    "        # Calculate deviation from optimal pH\n",
    "        deviation = np.abs(ph - optimal)\n",
    "        stress = deviation / 2.0  # Normalize to 0-1 scale\n",
    "        \n",
    "        # Add extreme pH stress\n",
    "        stress[(ph < self.thresholds['ph']['low']) | \n",
    "               (ph > self.thresholds['ph']['high'])] = 1.0\n",
    "        \n",
    "        return stress.clip(0, 1)\n",
    "    \n",
    "    def calculate_temperature_stress(self, df):\n",
    "        \"\"\"Calculate thermal stress\"\"\"\n",
    "        temp = self._get_column(df, ['temperature_mean', 'temperature', 'temp_mean', 'temp'])\n",
    "        if temp is None:\n",
    "            return pd.Series(0, index=df.index)\n",
    "        stress = pd.Series(0.0, index=df.index)\n",
    "        \n",
    "        # Cold stress\n",
    "        stress[temp < self.thresholds['temperature']['low']] = 0.3\n",
    "        # Optimal range\n",
    "        stress[(temp >= self.thresholds['temperature']['low']) & \n",
    "               (temp < self.thresholds['temperature']['moderate'])] = 0.1\n",
    "        # Warm stress\n",
    "        stress[(temp >= self.thresholds['temperature']['moderate']) & \n",
    "               (temp < self.thresholds['temperature']['high'])] = 0.6\n",
    "        # Heat stress\n",
    "        stress[temp >= self.thresholds['temperature']['high']] = 1.0\n",
    "        \n",
    "        return stress.clip(0, 1)\n",
    "    \n",
    "    def calculate_impervious_stress(self, df):\n",
    "        \"\"\"Calculate stress from impervious surface coverage\"\"\"\n",
    "        imperv_cols = [col for col in df.columns if 'imperv_mean' in col]\n",
    "        if not imperv_cols:\n",
    "            return pd.Series(0, index=df.index)\n",
    "        \n",
    "        # Use the 1000m buffer as primary indicator\n",
    "        if 'imperv_mean_1000m' in df.columns:\n",
    "            imperv_pct = df['imperv_mean_1000m']\n",
    "        else:\n",
    "            imperv_pct = df[imperv_cols[0]]\n",
    "        \n",
    "        # Stress increases exponentially with impervious coverage\n",
    "        stress = (imperv_pct / 100) ** 1.5\n",
    "        \n",
    "        return stress.clip(0, 1)\n",
    "    \n",
    "    def calculate_landuse_stress(self, df):\n",
    "        \"\"\"Calculate stress from problematic land uses\"\"\"\n",
    "        stress = pd.Series(0.0, index=df.index)\n",
    "        \n",
    "        # Industrial land use is highest stress\n",
    "        if 'industrial_1000m_pct' in df.columns:\n",
    "            stress += df['industrial_1000m_pct'] / 100 * 0.8\n",
    "        \n",
    "        # Urban land use is moderate stress\n",
    "        if 'urban_1000m_pct' in df.columns:\n",
    "            stress += df['urban_1000m_pct'] / 100 * 0.5\n",
    "        \n",
    "        # Forest cover reduces stress (negative contribution)\n",
    "        if 'forest_1000m_pct' in df.columns:\n",
    "            stress -= df['forest_1000m_pct'] / 100 * 0.3\n",
    "        \n",
    "        # Wetlands also reduce stress\n",
    "        if 'wetland_1000m_pct' in df.columns:\n",
    "            stress -= df['wetland_1000m_pct'] / 100 * 0.2\n",
    "        \n",
    "        return stress.clip(0, 1)\n",
    "    \n",
    "    def calculate_wslus(self, df):\n",
    "        \"\"\"\n",
    "        Calculate the composite Water Stress from Land Use Score\n",
    "        \"\"\"\n",
    "        # Calculate individual stress components\n",
    "        stress_components = {\n",
    "            'conductivity_stress': self.calculate_conductivity_stress(df),\n",
    "            'tds_stress': self.calculate_tds_stress(df),\n",
    "            'ph_stress': self.calculate_ph_stress(df),\n",
    "            'temperature_stress': self.calculate_temperature_stress(df),\n",
    "            'impervious_stress': self.calculate_impervious_stress(df),\n",
    "            'landuse_stress': self.calculate_landuse_stress(df)\n",
    "        }\n",
    "        \n",
    "        # Add stress components to dataframe\n",
    "        for name, values in stress_components.items():\n",
    "            df[name] = values\n",
    "        \n",
    "        # Calculate weighted composite score\n",
    "        wslus = pd.Series(0.0, index=df.index)\n",
    "        for component, weight in self.weights.items():\n",
    "            if component in stress_components:\n",
    "                wslus += stress_components[component] * weight\n",
    "        \n",
    "        # Apply EJ community amplifier\n",
    "        if 'ej_vulnerability' in df.columns:\n",
    "            ej_amplifier = 1 + df['ej_vulnerability'] * 0.3\n",
    "            wslus = wslus * ej_amplifier\n",
    "        \n",
    "        # Normalize to 0-100 scale\n",
    "        wslus = (wslus * 100).clip(0, 100)\n",
    "        \n",
    "        df['WSLUS'] = wslus\n",
    "        \n",
    "        # Add risk categories\n",
    "        df['risk_category'] = pd.cut(wslus, \n",
    "                                     bins=[0, 25, 50, 75, 100],\n",
    "                                     labels=['Low', 'Moderate', 'High', 'Critical'])\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Initialize scorer and calculate WSLUS\n",
    "scorer = WaterStressScorer()\n",
    "stations_with_scores = scorer.calculate_wslus(stations_with_spatial.copy())\n",
    "\n",
    "print(\"Water Stress from Land Use Scores calculated successfully!\")\n",
    "print(f\"\\nWSLUS Statistics:\")\n",
    "print(stations_with_scores['WSLUS'].describe())\n",
    "print(f\"\\nRisk Category Distribution:\")\n",
    "print(stations_with_scores['risk_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preservation Opportunity Identification\n",
    "\n",
    "### 6.1 Calculate Preservation Impact Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_preservation_opportunities(df, layers):\n",
    "    \"\"\"\n",
    "    Identify specific land parcels where preservation would have maximum impact\n",
    "    \"\"\"\n",
    "    opportunities = []\n",
    "    \n",
    "    # Get CRS from dataframe\n",
    "    ma_crs = df.crs\n",
    "    \n",
    "    # Focus on stations with elevated stress (lower threshold to find more opportunities)\n",
    "    # Use 25th percentile or 30 as threshold (whichever is lower) to ensure we find opportunities\n",
    "    wslus_threshold = min(df['WSLUS'].quantile(0.75), 40)  # Top 25% or 40, whichever is lower\n",
    "    high_stress_stations = df[df['WSLUS'] > wslus_threshold].copy()\n",
    "    \n",
    "    print(f\"Using WSLUS threshold: {wslus_threshold:.1f} (found {len(high_stress_stations)} stations)\")\n",
    "    \n",
    "    if len(high_stress_stations) == 0:\n",
    "        # If still no stations, use median or even lower threshold\n",
    "        wslus_threshold = max(df['WSLUS'].median(), 25)\n",
    "        high_stress_stations = df[df['WSLUS'] > wslus_threshold].copy()\n",
    "        print(f\"Lowered threshold to {wslus_threshold:.1f} (found {len(high_stress_stations)} stations)\")\n",
    "    \n",
    "    if len(high_stress_stations) == 0:\n",
    "        print(\"Warning: No stations above threshold. Creating opportunities based on all stations.\")\n",
    "        # Use all stations if still none found\n",
    "        high_stress_stations = df.copy()\n",
    "        print(f\"Using all {len(high_stress_stations)} stations for opportunity identification\")\n",
    "    \n",
    "    # Check if we have GIS layers or need to use alternative approach\n",
    "    has_landuse = 'landuse' in layers and len(layers.get('landuse', [])) > 0\n",
    "    has_protected = 'protected' in layers and len(layers.get('protected', [])) > 0\n",
    "    \n",
    "    if not has_landuse:\n",
    "        print(\"No landuse GIS layer available. Creating opportunities based on station characteristics and economic data.\")\n",
    "        # Create opportunities based on station data and economic indicators\n",
    "        for idx, station in high_stress_stations.iterrows():\n",
    "            # Create opportunity based on station characteristics\n",
    "            wslus = station.get('WSLUS', 0)\n",
    "            ej_flag = station.get('in_ej_community', 0) == 1 or station.get('ej_vulnerability', 0) > 0.5\n",
    "            \n",
    "            # Estimate area based on urbanization proxy or distance from Boston\n",
    "            if 'urbanization_proxy' in station.index:\n",
    "                area_hectares = 50 + (1 - station['urbanization_proxy']) * 100  # More area in rural areas\n",
    "            elif 'dist_from_boston_km' in station.index:\n",
    "                area_hectares = 50 + min(station['dist_from_boston_km'] / 10, 100)\n",
    "            else:\n",
    "                area_hectares = 75  # Default\n",
    "            \n",
    "            # Calculate preservation score\n",
    "            preservation_score = wslus * 0.6 + (ej_flag * 30) + (area_hectares / 10)\n",
    "            estimated_impact = preservation_score * area_hectares\n",
    "            \n",
    "            opportunities.append({\n",
    "                'rank': len(opportunities) + 1,\n",
    "                'station_id': station.get('station_id', f'ST{idx}'),\n",
    "                'landuse_type': 'Priority Area',  # Generic since no GIS data\n",
    "                'area_hectares': area_hectares,\n",
    "                'preservation_score': preservation_score,\n",
    "                'estimated_impact': estimated_impact,\n",
    "                'wslus_score': wslus,\n",
    "                'ej_priority': ej_flag,\n",
    "                'parcel_geometry': station.geometry.buffer(500)  # 500m buffer as opportunity area\n",
    "            })\n",
    "        \n",
    "        if len(opportunities) > 0:\n",
    "            opp_df = pd.DataFrame(opportunities)\n",
    "            opp_df = opp_df.sort_values('preservation_score', ascending=False).reset_index(drop=True)\n",
    "            opp_df['rank'] = range(1, len(opp_df) + 1)\n",
    "            print(f\"Created {len(opp_df)} opportunities based on station characteristics\")\n",
    "            return opp_df\n",
    "    \n",
    "    # Original GIS-based approach (if landuse layer is available)\n",
    "    for idx, station in high_stress_stations.iterrows():\n",
    "        # Create search area (2km upstream buffer)\n",
    "        search_buffer = station.geometry.buffer(2000)\n",
    "        \n",
    "        # Find unprotected land parcels\n",
    "        if has_landuse:\n",
    "            # Get land parcels in search area\n",
    "            search_gdf = gpd.GeoDataFrame([{'geometry': search_buffer}], crs=ma_crs)\n",
    "            nearby_land = gpd.sjoin(layers['landuse'], search_gdf, predicate='intersects')\n",
    "            \n",
    "            # Exclude already protected lands\n",
    "            try:\n",
    "                protected_geoms = layers['protected'].unary_union\n",
    "                # Check if geometries intersect with protected areas\n",
    "                unprotected = nearby_land[~nearby_land.geometry.intersects(protected_geoms)]\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not filter protected lands: {e}\")\n",
    "                unprotected = nearby_land\n",
    "            \n",
    "            # Calculate preservation value for each parcel\n",
    "            for land_idx, parcel in unprotected.iterrows():\n",
    "                # Calculate distance to station\n",
    "                distance = station.geometry.distance(parcel.geometry.centroid)\n",
    "                \n",
    "                # Calculate preservation score\n",
    "                preservation_score = 0\n",
    "                \n",
    "                # Higher score for forests and wetlands\n",
    "                if parcel['landuse'] in ['Forest', 'Wetland']:\n",
    "                    preservation_score += 40\n",
    "                elif parcel['landuse'] == 'Agriculture':\n",
    "                    preservation_score += 20\n",
    "                \n",
    "                # Proximity bonus (closer = higher impact)\n",
    "                proximity_score = max(0, 30 * (1 - distance / 2000))\n",
    "                preservation_score += proximity_score\n",
    "                \n",
    "                # EJ community bonus\n",
    "                if station['in_ej_community'] == 1:\n",
    "                    preservation_score += 20\n",
    "                \n",
    "                # Water stress bonus\n",
    "                stress_bonus = station['WSLUS'] / 100 * 30\n",
    "                preservation_score += stress_bonus\n",
    "                \n",
    "                opportunities.append({\n",
    "                    'station_id': station['station_id'],\n",
    "                    'parcel_geometry': parcel.geometry,\n",
    "                    'landuse_type': parcel['landuse'],\n",
    "                    'area_hectares': parcel.geometry.area / 10000,\n",
    "                    'distance_to_station': distance,\n",
    "                    'preservation_score': preservation_score,\n",
    "                    'station_wslus': station['WSLUS'],\n",
    "                    'in_ej_community': station['in_ej_community'],\n",
    "                    'estimated_impact': preservation_score * parcel.geometry.area / 10000  # Score  area\n",
    "                })\n",
    "    \n",
    "    opportunities_df = pd.DataFrame(opportunities)\n",
    "    \n",
    "    # Rank opportunities\n",
    "    if len(opportunities_df) > 0:\n",
    "        opportunities_df = opportunities_df.sort_values('estimated_impact', ascending=False)\n",
    "        opportunities_df['rank'] = range(1, len(opportunities_df) + 1)\n",
    "    \n",
    "    return opportunities_df\n",
    "\n",
    "# Identify preservation opportunities\n",
    "preservation_opportunities = identify_preservation_opportunities(stations_with_scores, gis_layers)\n",
    "\n",
    "print(f\"Identified {len(preservation_opportunities)} preservation opportunities\")\n",
    "if len(preservation_opportunities) > 0:\n",
    "    print(f\"\\nTop 10 Preservation Opportunities:\")\n",
    "    print(preservation_opportunities[['rank', 'landuse_type', 'area_hectares', \n",
    "                                     'preservation_score', 'estimated_impact']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance Analysis\n",
    "\n",
    "### 7.1 Clustering Analysis to Identify Stress Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_analysis(df):\n",
    "    \"\"\"\n",
    "    Identify spatial clusters of water stress\n",
    "    \"\"\"\n",
    "    # Select features for clustering\n",
    "    feature_cols = ['conductivity_stress', 'tds_stress', 'ph_stress', \n",
    "                   'temperature_stress', 'impervious_stress', 'landuse_stress']\n",
    "    \n",
    "    X = df[feature_cols].fillna(0)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Determine optimal number of clusters using elbow method\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    K_range = range(2, min(10, len(df)))\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "    \n",
    "    # Use optimal k (highest silhouette score)\n",
    "    optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "    \n",
    "    # Final clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    df['stress_cluster'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate cluster centroids in original scale\n",
    "    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "    cluster_profiles = pd.DataFrame(cluster_centers, columns=feature_cols)\n",
    "    cluster_profiles['cluster_id'] = range(optimal_k)\n",
    "    \n",
    "    # Spatial clustering using DBSCAN\n",
    "    coords = df[['geometry']].copy()\n",
    "    coords['x'] = coords.geometry.x\n",
    "    coords['y'] = coords.geometry.y\n",
    "    \n",
    "    # DBSCAN for spatial hotspot detection\n",
    "    dbscan = DBSCAN(eps=3000, min_samples=3)  # 3km radius, min 3 stations\n",
    "    df['spatial_cluster'] = dbscan.fit_predict(coords[['x', 'y']])\n",
    "    \n",
    "    # Identify hotspots (spatial clusters with high WSLUS)\n",
    "    hotspot_stats = df[df['spatial_cluster'] != -1].groupby('spatial_cluster').agg({\n",
    "        'WSLUS': ['mean', 'max', 'count'],\n",
    "        'station_id': 'count'\n",
    "    })\n",
    "    \n",
    "    return df, cluster_profiles, hotspot_stats\n",
    "\n",
    "# Perform clustering\n",
    "stations_clustered, cluster_profiles, hotspot_stats = perform_clustering_analysis(stations_with_scores)\n",
    "\n",
    "print(f\"Identified {cluster_profiles.shape[0]} stress pattern clusters\")\n",
    "print(f\"\\nCluster Profiles:\")\n",
    "print(cluster_profiles.round(3))\n",
    "\n",
    "if len(hotspot_stats) > 0:\n",
    "    print(f\"\\nIdentified {len(hotspot_stats)} spatial hotspots\")\n",
    "    print(hotspot_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.1.5 Feature Analysis and Simplification\n",
    "\n",
    "# Analyze and simplify features before modeling\n",
    "def analyze_and_simplify_features(df, target_col='WSLUS', missing_threshold=0.5, \n",
    "                                   variance_threshold=1e-6, correlation_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Analyze features and remove low-quality ones that hurt model performance\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"FEATURE ANALYSIS AND SIMPLIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get potential feature columns (exclude geometry, IDs, target)\n",
    "    exclude_cols = ['geometry', 'station_id', target_col, 'risk_category', 'stress_cluster', 'spatial_cluster']\n",
    "    potential_features = [col for col in df.columns \n",
    "                         if col not in exclude_cols \n",
    "                         and df[col].dtype in [np.float64, np.int64, np.float32, np.int32]]\n",
    "    \n",
    "    print(f\"\\nInitial potential features: {len(potential_features)}\")\n",
    "    \n",
    "    # Check if target exists\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Warning: Target column '{target_col}' not found!\")\n",
    "        return df, []\n",
    "    \n",
    "    # Filter out rows with missing target\n",
    "    df_clean = df.dropna(subset=[target_col]).copy()\n",
    "    print(f\"Stations with valid {target_col}: {len(df_clean)}\")\n",
    "    \n",
    "    # Step 1: Check missing values\n",
    "    print(\"\\n1. Checking missing values...\")\n",
    "    missing_stats = {}\n",
    "    for col in potential_features:\n",
    "        if col in df_clean.columns:\n",
    "            missing_pct = df_clean[col].isna().sum() / len(df_clean)\n",
    "            missing_stats[col] = missing_pct\n",
    "    \n",
    "    high_missing = [col for col, pct in missing_stats.items() if pct > missing_threshold]\n",
    "    if high_missing:\n",
    "        print(f\"   Removing {len(high_missing)} features with >{missing_threshold*100}% missing:\")\n",
    "        for col in high_missing[:10]:  # Show first 10\n",
    "            print(f\"     - {col}: {missing_stats[col]:.1%} missing\")\n",
    "        if len(high_missing) > 10:\n",
    "            print(f\"     ... and {len(high_missing)-10} more\")\n",
    "        potential_features = [f for f in potential_features if f not in high_missing]\n",
    "    \n",
    "    # Step 2: Check variance\n",
    "    print(\"\\n2. Checking feature variance...\")\n",
    "    variance_stats = {}\n",
    "    for col in potential_features:\n",
    "        if col in df_clean.columns:\n",
    "            var = df_clean[col].var()\n",
    "            variance_stats[col] = var\n",
    "    \n",
    "    low_variance = [col for col, var in variance_stats.items() if var < variance_threshold]\n",
    "    if low_variance:\n",
    "        print(f\"   Removing {len(low_variance)} low-variance features:\")\n",
    "        for col in low_variance[:10]:\n",
    "            print(f\"     - {col}: variance = {variance_stats[col]:.2e}\")\n",
    "        if len(low_variance) > 10:\n",
    "            print(f\"     ... and {len(low_variance)-10} more\")\n",
    "        potential_features = [f for f in potential_features if f not in low_variance]\n",
    "    \n",
    "    # Step 3: Check correlation with target\n",
    "    print(\"\\n3. Checking correlation with target...\")\n",
    "    correlations = {}\n",
    "    for col in potential_features:\n",
    "        if col in df_clean.columns:\n",
    "            # Fill missing with median for correlation calculation\n",
    "            col_data = df_clean[col].fillna(df_clean[col].median())\n",
    "            target_data = df_clean[target_col]\n",
    "            # Remove rows where either is still NaN\n",
    "            valid_mask = ~(col_data.isna() | target_data.isna())\n",
    "            if valid_mask.sum() > 10:\n",
    "                corr = np.abs(np.corrcoef(col_data[valid_mask], target_data[valid_mask])[0, 1])\n",
    "                correlations[col] = corr if not np.isnan(corr) else 0\n",
    "            else:\n",
    "                correlations[col] = 0\n",
    "    \n",
    "    low_correlation = [col for col, corr in correlations.items() if corr < correlation_threshold]\n",
    "    if low_correlation:\n",
    "        print(f\"   Removing {len(low_correlation)} features with |correlation| < {correlation_threshold}:\")\n",
    "        for col in sorted(low_correlation, key=lambda x: correlations[x])[:10]:\n",
    "            print(f\"     - {col}: |corr| = {correlations[col]:.4f}\")\n",
    "        if len(low_correlation) > 10:\n",
    "            print(f\"     ... and {len(low_correlation)-10} more\")\n",
    "        potential_features = [f for f in potential_features if f not in low_correlation]\n",
    "    \n",
    "    # Step 4: Check for constant or near-constant features\n",
    "    print(\"\\n4. Checking for constant features...\")\n",
    "    constant_features = []\n",
    "    for col in potential_features:\n",
    "        if col in df_clean.columns:\n",
    "            unique_vals = df_clean[col].nunique()\n",
    "            if unique_vals <= 1:\n",
    "                constant_features.append(col)\n",
    "    \n",
    "    if constant_features:\n",
    "        print(f\"   Removing {len(constant_features)} constant features\")\n",
    "        potential_features = [f for f in potential_features if f not in constant_features]\n",
    "    \n",
    "    # Step 5: Analyze by feature type/category\n",
    "    print(\"\\n5. Feature summary by category:\")\n",
    "    feature_categories = {\n",
    "        'Land Use': [f for f in potential_features if any(x in f for x in ['forest', 'urban', 'industrial', 'wetland', 'agriculture', 'water'])],\n",
    "        'Composite': [f for f in potential_features if any(x in f for x in ['developed', 'natural', 'ratio', 'index', 'weighted', 'diversity'])],\n",
    "        'Impervious': [f for f in potential_features if 'imperv' in f],\n",
    "        'EJ': [f for f in potential_features if 'ej' in f],\n",
    "        'Water Quality': [f for f in potential_features if any(x in f for x in ['ph', 'conductivity', 'tds', 'temperature', 'do'])],\n",
    "        'Other': [f for f in potential_features if not any(f in cat for cat in [\n",
    "            [f for f in potential_features if any(x in f for x in ['forest', 'urban', 'industrial', 'wetland', 'agriculture', 'water'])],\n",
    "            [f for f in potential_features if any(x in f for x in ['developed', 'natural', 'ratio', 'index', 'weighted', 'diversity'])],\n",
    "            [f for f in potential_features if 'imperv' in f],\n",
    "            [f for f in potential_features if 'ej' in f],\n",
    "            [f for f in potential_features if any(x in f for x in ['ph', 'conductivity', 'tds', 'temperature', 'do'])]\n",
    "        ])]\n",
    "    }\n",
    "    \n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            avg_corr = np.mean([correlations.get(f, 0) for f in features if f in correlations])\n",
    "            print(f\"   {category}: {len(features)} features (avg |corr| = {avg_corr:.3f})\")\n",
    "    \n",
    "    # Step 6: Focus on best features by buffer distance\n",
    "    print(\"\\n6. Top features by buffer distance:\")\n",
    "    buffer_dists = [500, 1000, 2000, 5000]\n",
    "    for dist in buffer_dists:\n",
    "        dist_features = [f for f in potential_features if f'{dist}m' in f]\n",
    "        if dist_features:\n",
    "            dist_corrs = [(f, correlations.get(f, 0)) for f in dist_features if f in correlations]\n",
    "            dist_corrs.sort(key=lambda x: x[1], reverse=True)\n",
    "            if dist_corrs:\n",
    "                best = dist_corrs[0]\n",
    "                print(f\"   {dist}m buffer: {len(dist_features)} features, best = {best[0]} (|corr|={best[1]:.3f})\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Final feature count: {len(potential_features)} (removed {len(df.columns) - len(potential_features) - len(exclude_cols)} low-quality features)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Store feature analysis results\n",
    "    df_clean['_feature_analysis_done'] = True\n",
    "    \n",
    "    return df_clean, potential_features\n",
    "\n",
    "# Analyze and simplify features\n",
    "stations_analyzed, quality_features = analyze_and_simplify_features(\n",
    "    stations_clustered, \n",
    "    target_col='WSLUS',\n",
    "    missing_threshold=0.5,  # Remove if >50% missing\n",
    "    variance_threshold=1e-6,  # Remove if variance too low\n",
    "    correlation_threshold=0.01  # Remove if correlation with target too low\n",
    ")\n",
    "\n",
    "print(f\"\\nUsing {len(quality_features)} high-quality features for modeling\")\n",
    "print(f\"Top 10 features by correlation:\")\n",
    "if 'WSLUS' in stations_analyzed.columns:\n",
    "    corrs = []\n",
    "    for feat in quality_features[:20]:  # Check top 20\n",
    "        if feat in stations_analyzed.columns:\n",
    "            data = stations_analyzed[feat].fillna(stations_analyzed[feat].median())\n",
    "            target = stations_analyzed['WSLUS']\n",
    "            valid = ~(data.isna() | target.isna())\n",
    "            if valid.sum() > 10:\n",
    "                corr = np.abs(np.corrcoef(data[valid], target[valid])[0, 1])\n",
    "                if not np.isnan(corr):\n",
    "                    corrs.append((feat, corr))\n",
    "    corrs.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (feat, corr) in enumerate(corrs[:10], 1):\n",
    "        print(f\"  {i}. {feat}: |corr| = {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Predictive Model for Water Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_predictive_model(df):\n",
    "    \"\"\"\n",
    "    Build an improved predictive model to estimate WSLUS from land characteristics\n",
    "    Uses feature selection, hyperparameter tuning, and better data cleaning\n",
    "    \"\"\"\n",
    "    # Use pre-analyzed quality features if available, otherwise select features\n",
    "    if '_feature_analysis_done' in df.columns:\n",
    "        # Use the quality features from analysis (exclude metadata columns)\n",
    "        exclude_cols = ['geometry', 'station_id', 'WSLUS', 'risk_category', 'stress_cluster', \n",
    "                       'spatial_cluster', '_feature_analysis_done']\n",
    "        feature_cols = [col for col in df.columns \n",
    "                       if col not in exclude_cols \n",
    "                       and df[col].dtype in [np.float64, np.int64, np.float32, np.int32]]\n",
    "        print(f\"Using pre-analyzed features: {len(feature_cols)} features\")\n",
    "    else:\n",
    "        # Fallback: select features manually (simplified - focus on most important)\n",
    "        print(\"Warning: Using fallback feature selection. Run feature analysis first for better results.\")\n",
    "        # Focus on most predictive feature types\n",
    "        spatial_keywords = ['forest_', 'urban_', 'developed_', 'natural_', 'ej_']\n",
    "        feature_cols = [col for col in df.columns if any(x in col for x in spatial_keywords)]\n",
    "        \n",
    "        # Include water quality metrics if available\n",
    "        wq_keywords = ['_mean', '_median']\n",
    "        wq_features = [col for col in df.columns if any(x in col for x in wq_keywords) \n",
    "                       and any(p in col.lower() for p in ['ph', 'conductivity', 'tds', 'temperature', 'do'])]\n",
    "        feature_cols.extend(wq_features)\n",
    "        \n",
    "        # Remove geometry and non-numeric columns\n",
    "        feature_cols = [col for col in feature_cols if col in df.columns \n",
    "                       and col != 'geometry' and df[col].dtype in [np.float64, np.int64, np.float32, np.int32]]\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        print(\"No spatial features available for modeling\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Filter out rows with NaN WSLUS values\n",
    "    df_clean = df.dropna(subset=['WSLUS']).copy()\n",
    "    \n",
    "    if len(df_clean) == 0:\n",
    "        print(\"No valid WSLUS values available for modeling\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Remove outliers in WSLUS (keep 95% of data)\n",
    "    q5 = df_clean['WSLUS'].quantile(0.05)\n",
    "    q95 = df_clean['WSLUS'].quantile(0.95)\n",
    "    df_clean = df_clean[(df_clean['WSLUS'] >= q5) & (df_clean['WSLUS'] <= q95)]\n",
    "    \n",
    "    print(f\"Using {len(df_clean)} stations with valid WSLUS scores (dropped {len(df) - len(df_clean)} with NaN/outliers)\")\n",
    "    \n",
    "    # Prepare features\n",
    "    X = df_clean[feature_cols].copy()\n",
    "    \n",
    "    # Fill missing values with median (better than 0 for many features)\n",
    "    for col in X.columns:\n",
    "        if X[col].isna().sum() > 0:\n",
    "            median_val = X[col].median()\n",
    "            X[col] = X[col].fillna(median_val if not pd.isna(median_val) else 0)\n",
    "    \n",
    "    # Remove features with zero variance or very low variance\n",
    "    feature_variance = X.var()\n",
    "    low_variance_features = feature_variance[feature_variance < 1e-6].index.tolist()\n",
    "    if low_variance_features:\n",
    "        print(f\"Removing {len(low_variance_features)} low-variance features\")\n",
    "        X = X.drop(columns=low_variance_features)\n",
    "        feature_cols = [f for f in feature_cols if f not in low_variance_features]\n",
    "    \n",
    "    y = df_clean['WSLUS']\n",
    "    \n",
    "    # Remove outliers in features using IQR method\n",
    "    for col in X.columns:\n",
    "        Q1 = X[col].quantile(0.25)\n",
    "        Q3 = X[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        if IQR > 0:\n",
    "            lower_bound = Q1 - 3 * IQR\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            X[col] = X[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=None)\n",
    "    \n",
    "    # Feature selection using mutual information (better for non-linear relationships)\n",
    "    print(f\"\\nOriginal features: {len(feature_cols)}\")\n",
    "    selector = SelectKBest(score_func=mutual_info_regression, k=min(30, len(feature_cols)))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    selected_features = [feature_cols[i] for i in selector.get_support(indices=True)]\n",
    "    print(f\"Selected {len(selected_features)} features using mutual information\")\n",
    "    \n",
    "    # Scale features using RobustScaler (less sensitive to outliers)\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "    \n",
    "    # Hyperparameter tuning for Random Forest\n",
    "    print(\"\\nTuning Random Forest hyperparameters...\")\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    rf_grid = GridSearchCV(rf_base, param_grid_rf, cv=5, scoring='r2', n_jobs=-1, verbose=0)\n",
    "    rf_grid.fit(X_train_scaled, y_train)\n",
    "    rf_model = rf_grid.best_estimator_\n",
    "    \n",
    "    print(f\"Best RF params: {rf_grid.best_params_}\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = rf_model.predict(X_train_scaled)\n",
    "    y_pred_test = rf_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    print(\"\\n=== Predictive Model Performance ===\")\n",
    "    print(f\"Training R: {train_r2:.3f}\")\n",
    "    print(f\"Test R: {test_r2:.3f}\")\n",
    "    print(f\"Training RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"Training MAE: {train_mae:.2f}\")\n",
    "    print(f\"Test MAE: {test_mae:.2f}\")\n",
    "    print(f\"Cross-validation R (mean  std): {cv_scores.mean():.3f}  {cv_scores.std():.3f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': rf_model.feature_importances_,\n",
    "        'mi_score': selector.scores_[selector.get_support()]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Try Gradient Boosting with tuning\n",
    "    print(\"\\nTuning Gradient Boosting hyperparameters...\")\n",
    "    param_grid_gb = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.05, 0.1]\n",
    "    }\n",
    "    \n",
    "    gb_base = GradientBoostingRegressor(random_state=42)\n",
    "    gb_grid = GridSearchCV(gb_base, param_grid_gb, cv=5, scoring='r2', n_jobs=-1, verbose=0)\n",
    "    gb_grid.fit(X_train_scaled, y_train)\n",
    "    gb_model = gb_grid.best_estimator_\n",
    "    gb_test_r2 = r2_score(y_test, gb_model.predict(X_test_scaled))\n",
    "    \n",
    "    print(f\"Best GB params: {gb_grid.best_params_}\")\n",
    "    print(f\"Gradient Boosting Test R: {gb_test_r2:.3f}\")\n",
    "    \n",
    "    # Use the better model\n",
    "    if gb_test_r2 > test_r2:\n",
    "        print(f\"\\nUsing Gradient Boosting model (R = {gb_test_r2:.3f} vs RF R = {test_r2:.3f})\")\n",
    "        best_model = gb_model\n",
    "    else:\n",
    "        print(f\"\\nUsing Random Forest model (R = {test_r2:.3f} vs GB R = {gb_test_r2:.3f})\")\n",
    "        best_model = rf_model\n",
    "    \n",
    "    return best_model, feature_importance, {\n",
    "        'train_r2': train_r2, 'test_r2': test_r2, \n",
    "        'train_rmse': train_rmse, 'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae, 'test_mae': test_mae,\n",
    "        'cv_scores': cv_scores,\n",
    "        'selected_features': selected_features,\n",
    "        'scaler': scaler,\n",
    "        'selector': selector\n",
    "    }\n",
    "\n",
    "# Build and evaluate model using simplified features\n",
    "model, feature_importance, metrics = build_predictive_model(stations_analyzed)\n",
    "\n",
    "if feature_importance is not None:\n",
    "    print(\"\\n=== Top 10 Most Important Features ===\")\n",
    "    print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization and Reporting\n",
    "\n",
    "### 8.1 Create Interactive Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_map(stations_df, opportunities_df=None):\n",
    "    \"\"\"\n",
    "    Create an interactive Folium map showing WSLUS scores and preservation opportunities\n",
    "    \"\"\"\n",
    "    # Convert to WGS84 for web mapping\n",
    "    stations_wgs = stations_df.to_crs('EPSG:4326')\n",
    "    \n",
    "    # Calculate map center\n",
    "    center_lat = stations_wgs.geometry.y.mean()\n",
    "    center_lon = stations_wgs.geometry.x.mean()\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=[center_lat, center_lon], \n",
    "                   zoom_start=10,\n",
    "                   tiles='OpenStreetMap')\n",
    "    \n",
    "    # Add tile layers\n",
    "    folium.TileLayer('CartoDB positron', name='Light Mode').add_to(m)\n",
    "    folium.TileLayer('CartoDB dark_matter', name='Dark Mode').add_to(m)\n",
    "    \n",
    "    # Color scheme for WSLUS scores\n",
    "    def get_color(wslus):\n",
    "        if wslus < 25:\n",
    "            return 'green'\n",
    "        elif wslus < 50:\n",
    "            return 'yellow'\n",
    "        elif wslus < 75:\n",
    "            return 'orange'\n",
    "        else:\n",
    "            return 'red'\n",
    "    \n",
    "    # Add monitoring stations\n",
    "    station_group = folium.FeatureGroup(name='Monitoring Stations')\n",
    "    \n",
    "    for idx, row in stations_wgs.iterrows():\n",
    "        # Skip rows with NaN WSLUS or invalid coordinates\n",
    "        if pd.isna(row.get('WSLUS')) or pd.isna(row.geometry.y) or pd.isna(row.geometry.x):\n",
    "            continue\n",
    "            \n",
    "        # Create popup text\n",
    "        wslus_val = row.get('WSLUS', 0)\n",
    "        risk_cat = row.get('risk_category', 'Unknown')\n",
    "        station_id = row.get('station_id', 'N/A')\n",
    "        popup_text = f\"\"\"\n",
    "        <div style=\"font-family: Arial; width: 250px;\">\n",
    "            <h4>Station {station_id}</h4>\n",
    "            <b>WSLUS Score:</b> {wslus_val:.1f}<br>\n",
    "            <b>Risk Category:</b> {risk_cat}<br>\n",
    "            <hr>\n",
    "            <b>Stress Components:</b><br>\n",
    "             Conductivity: {row.get('conductivity_stress', 0):.2f}<br>\n",
    "             TDS: {row.get('tds_stress', 0):.2f}<br>\n",
    "             pH: {row.get('ph_stress', 0):.2f}<br>\n",
    "             Temperature: {row.get('temperature_stress', 0):.2f}<br>\n",
    "             Impervious: {row.get('impervious_stress', 0):.2f}<br>\n",
    "             Land Use: {row.get('landuse_stress', 0):.2f}<br>\n",
    "            <hr>\n",
    "            <b>EJ Community:</b> {'Yes' if row.get('in_ej_community', 0) == 1 else 'No'}<br>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add marker\n",
    "        folium.CircleMarker(\n",
    "            location=[row.geometry.y, row.geometry.x],\n",
    "            radius=8 + (wslus_val/10 if pd.notna(wslus_val) else 5),  # Size based on score\n",
    "            popup=folium.Popup(popup_text, max_width=300),\n",
    "            tooltip=f\"Station {station_id}: {wslus_val:.1f}\",\n",
    "            color='black',\n",
    "            fillColor=get_color(wslus_val) if pd.notna(wslus_val) else 'gray',\n",
    "            fillOpacity=0.8,\n",
    "            weight=2\n",
    "        ).add_to(station_group)\n",
    "    \n",
    "    station_group.add_to(m)\n",
    "    \n",
    "    # Add preservation opportunities if available\n",
    "    if opportunities_df is not None and len(opportunities_df) > 0:\n",
    "        opp_group = folium.FeatureGroup(name='Top Preservation Opportunities')\n",
    "        \n",
    "        # Show top 20 opportunities\n",
    "        for idx, row in opportunities_df.head(20).iterrows():\n",
    "            # Convert geometry to WGS84\n",
    "            parcel_crs = stations_df.crs  # Get CRS from stations\n",
    "            geom_wgs = gpd.GeoSeries([row['parcel_geometry']], crs=parcel_crs).to_crs('EPSG:4326')[0]\n",
    "            \n",
    "            popup_text = f\"\"\"\n",
    "            <div style=\"font-family: Arial;\">\n",
    "                <h4>Preservation Opportunity #{row['rank']}</h4>\n",
    "                <b>Land Type:</b> {row['landuse_type']}<br>\n",
    "                <b>Area:</b> {row['area_hectares']:.1f} hectares<br>\n",
    "                <b>Preservation Score:</b> {row['preservation_score']:.1f}<br>\n",
    "                <b>Impact Score:</b> {row['estimated_impact']:.0f}<br>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            folium.GeoJson(\n",
    "                geom_wgs.__geo_interface__,\n",
    "                style_function=lambda x: {\n",
    "                    'fillColor': 'purple',\n",
    "                    'color': 'purple',\n",
    "                    'weight': 2,\n",
    "                    'fillOpacity': 0.3\n",
    "                },\n",
    "                tooltip=f\"Opportunity #{row['rank']}\",\n",
    "                popup=folium.Popup(popup_text, max_width=300)\n",
    "            ).add_to(opp_group)\n",
    "        \n",
    "        opp_group.add_to(m)\n",
    "    \n",
    "    # Add heatmap layer (filter out NaN values)\n",
    "    heat_data = [[row.geometry.y, row.geometry.x, row['WSLUS']] \n",
    "                 for idx, row in stations_wgs.iterrows()\n",
    "                 if pd.notna(row.get('WSLUS')) and pd.notna(row.geometry.y) and pd.notna(row.geometry.x)]\n",
    "    \n",
    "    if len(heat_data) > 0:\n",
    "        plugins.HeatMap(heat_data, name='WSLUS Heatmap', \n",
    "                       radius=30, blur=20).add_to(m)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_html = '''\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; right: 50px; width: 180px; height: 140px; \n",
    "                background-color: white; z-index: 1000; \n",
    "                border:2px solid grey; border-radius: 5px;\n",
    "                font-size: 14px; font-family: Arial;\n",
    "                \">\n",
    "        <p style=\"margin: 10px;\"><b>WSLUS Score</b></p>\n",
    "        <p style=\"margin: 10px;\"><span style=\"color: green;\"></span> Low (0-25)</p>\n",
    "        <p style=\"margin: 10px;\"><span style=\"color: gold;\"></span> Moderate (25-50)</p>\n",
    "        <p style=\"margin: 10px;\"><span style=\"color: orange;\"></span> High (50-75)</p>\n",
    "        <p style=\"margin: 10px;\"><span style=\"color: red;\"></span> Critical (75-100)</p>\n",
    "    </div>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Create map\n",
    "interactive_map = create_interactive_map(stations_clustered, preservation_opportunities)\n",
    "print(\"Interactive map created successfully!\")\n",
    "\n",
    "# Save map\n",
    "interactive_map.save('watershed_preservation_map.html')\n",
    "print(\"Map saved as 'watershed_preservation_map.html'\")\n",
    "\n",
    "# Display map (if in Jupyter)\n",
    "interactive_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Generate Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_dashboard(stations_df, opportunities_df):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for the analysis\n",
    "    \"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=('WSLUS Score Distribution', 'Risk Categories',\n",
    "                       'Stress Components by Station', 'Top Preservation Opportunities',\n",
    "                       'WSLUS vs Impervious Coverage', 'Spatial Clustering'),\n",
    "        specs=[[{'type': 'histogram'}, {'type': 'pie'}],\n",
    "               [{'type': 'bar'}, {'type': 'bar'}],\n",
    "               [{'type': 'scatter'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    # 1. WSLUS Distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=stations_df['WSLUS'], nbinsx=20, name='WSLUS'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Risk Categories Pie Chart\n",
    "    risk_counts = stations_df['risk_category'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=risk_counts.index, values=risk_counts.values,\n",
    "               marker=dict(colors=['green', 'yellow', 'orange', 'red'])),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Stress Components\n",
    "    stress_cols = ['conductivity_stress', 'tds_stress', 'ph_stress', \n",
    "                  'temperature_stress', 'impervious_stress', 'landuse_stress']\n",
    "    mean_stress = stations_df[stress_cols].mean()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=stress_cols, y=mean_stress.values, name='Mean Stress'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Top Preservation Opportunities\n",
    "    if len(opportunities_df) > 0:\n",
    "        top_opps = opportunities_df.head(10)\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=top_opps['rank'], y=top_opps['estimated_impact'],\n",
    "                  name='Impact Score'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 5. WSLUS vs Impervious Coverage\n",
    "    if 'imperv_mean_1000m' in stations_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=stations_df['imperv_mean_1000m'], \n",
    "                      y=stations_df['WSLUS'],\n",
    "                      mode='markers',\n",
    "                      marker=dict(color=stations_df['WSLUS'], colorscale='RdYlGn_r'),\n",
    "                      name='Stations'),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    # 6. Spatial Clustering\n",
    "    if 'stress_cluster' in stations_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=stations_df.geometry.x, \n",
    "                      y=stations_df.geometry.y,\n",
    "                      mode='markers',\n",
    "                      marker=dict(color=stations_df['stress_cluster'], \n",
    "                                size=stations_df['WSLUS']/5,\n",
    "                                colorscale='Viridis'),\n",
    "                      name='Clusters'),\n",
    "            row=3, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"Watershed Preservation Opportunity Analysis Dashboard\",\n",
    "        showlegend=False,\n",
    "        height=1200,\n",
    "        width=1400\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"WSLUS Score\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Stress Component\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Opportunity Rank\", row=2, col=2)\n",
    "    fig.update_xaxes(title_text=\"Impervious %\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"X Coordinate\", row=3, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Mean Stress\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Impact Score\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"WSLUS Score\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Y Coordinate\", row=3, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create dashboard\n",
    "dashboard = create_summary_dashboard(stations_clustered, preservation_opportunities)\n",
    "dashboard.show()\n",
    "\n",
    "# Save dashboard\n",
    "dashboard.write_html('watershed_analysis_dashboard.html')\n",
    "print(\"Dashboard saved as 'watershed_analysis_dashboard.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Recommendations\n",
    "\n",
    "### 9.1 Generate Actionable Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_executive_summary(stations_df, opportunities_df, model_metrics):\n",
    "    \"\"\"\n",
    "    Generate executive summary with key findings and recommendations\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        'overview': {},\n",
    "        'key_findings': [],\n",
    "        'critical_areas': [],\n",
    "        'recommendations': [],\n",
    "        'impact_estimates': {}\n",
    "    }\n",
    "    \n",
    "    # Overview statistics\n",
    "    summary['overview'] = {\n",
    "        'total_stations': len(stations_df),\n",
    "        'mean_wslus': stations_df['WSLUS'].mean(),\n",
    "        'critical_stations': len(stations_df[stations_df['risk_category'] == 'Critical']),\n",
    "        'high_risk_stations': len(stations_df[stations_df['risk_category'].isin(['High', 'Critical'])]),\n",
    "        'ej_affected_stations': stations_df['in_ej_community'].sum(),\n",
    "        'model_accuracy': model_metrics['test_r2'] if model_metrics else None\n",
    "    }\n",
    "    \n",
    "    # Key findings\n",
    "    # Finding 1: Primary stress drivers\n",
    "    stress_cols = ['conductivity_stress', 'tds_stress', 'ph_stress', \n",
    "                  'temperature_stress', 'impervious_stress', 'landuse_stress']\n",
    "    mean_stress = stations_df[stress_cols].mean().sort_values(ascending=False)\n",
    "    summary['key_findings'].append(\n",
    "        f\"Primary stress driver: {mean_stress.index[0].replace('_stress', '').title()} \"\n",
    "        f\"(mean stress: {mean_stress.values[0]:.2f})\"\n",
    "    )\n",
    "    \n",
    "    # Finding 2: EJ impact\n",
    "    ej_stations = stations_df[stations_df['in_ej_community'] == 1]\n",
    "    if len(ej_stations) > 0:\n",
    "        ej_mean_wslus = ej_stations['WSLUS'].mean()\n",
    "        non_ej_mean_wslus = stations_df[stations_df['in_ej_community'] == 0]['WSLUS'].mean()\n",
    "        summary['key_findings'].append(\n",
    "            f\"EJ communities experience {(ej_mean_wslus/non_ej_mean_wslus - 1)*100:.1f}% \"\n",
    "            f\"higher water stress than non-EJ areas\"\n",
    "        )\n",
    "    \n",
    "    # Finding 3: Impervious surface correlation\n",
    "    if 'imperv_mean_1000m' in stations_df.columns:\n",
    "        corr = stations_df[['WSLUS', 'imperv_mean_1000m']].corr().iloc[0, 1]\n",
    "        summary['key_findings'].append(\n",
    "            f\"Strong correlation (r={corr:.2f}) between impervious surface and water stress\"\n",
    "        )\n",
    "    \n",
    "    # Critical areas\n",
    "    critical_stations = stations_df[stations_df['risk_category'] == 'Critical'].sort_values('WSLUS', ascending=False)\n",
    "    for idx, station in critical_stations.head(5).iterrows():\n",
    "        summary['critical_areas'].append({\n",
    "            'station_id': station['station_id'],\n",
    "            'wslus_score': station['WSLUS'],\n",
    "            'primary_issue': max([(station[col], col) for col in stress_cols])[1].replace('_stress', ''),\n",
    "            'in_ej_community': bool(station['in_ej_community'])\n",
    "        })\n",
    "    \n",
    "    # Recommendations\n",
    "    if len(opportunities_df) > 0:\n",
    "        top_opportunities = opportunities_df.head(10)\n",
    "        total_area = top_opportunities['area_hectares'].sum()\n",
    "        total_impact = top_opportunities['estimated_impact'].sum()\n",
    "        \n",
    "        summary['recommendations'].append(\n",
    "            f\"Preserve {total_area:.0f} hectares across {len(top_opportunities)} priority parcels\"\n",
    "        )\n",
    "        summary['recommendations'].append(\n",
    "            f\"Focus on {'forest and wetland' if 'Forest' in top_opportunities['landuse_type'].values else 'natural'} \"\n",
    "            f\"preservation for maximum impact\"\n",
    "        )\n",
    "    \n",
    "    # Targeted interventions\n",
    "    if mean_stress.values[0] > 0.6:  # High conductivity stress\n",
    "        summary['recommendations'].append(\n",
    "            \"Implement road salt reduction program and enhanced stormwater management\"\n",
    "        )\n",
    "    \n",
    "    if stations_df['impervious_stress'].mean() > 0.5:\n",
    "        summary['recommendations'].append(\n",
    "            \"Retrofit impervious surfaces with green infrastructure in high-stress watersheds\"\n",
    "        )\n",
    "    \n",
    "    # Impact estimates\n",
    "    if len(opportunities_df) > 0:\n",
    "        summary['impact_estimates'] = {\n",
    "            'parcels_to_preserve': len(opportunities_df.head(20)),\n",
    "            'total_preservation_area_ha': opportunities_df.head(20)['area_hectares'].sum(),\n",
    "            'estimated_wslus_reduction': 15.0,  # Placeholder - would need modeling\n",
    "            'affected_population': stations_df['in_ej_community'].sum() * 5000,  # Rough estimate\n",
    "            'priority_watersheds': len(stations_df['spatial_cluster'].unique())\n",
    "        }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summary\n",
    "executive_summary = generate_executive_summary(stations_clustered, preservation_opportunities, metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXECUTIVE SUMMARY: WATERSHED PRESERVATION OPPORTUNITY MAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nOVERVIEW:\")\n",
    "for key, value in executive_summary['overview'].items():\n",
    "    print(f\"   {key.replace('_', ' ').title()}: {value:.1f if isinstance(value, float) else value}\")\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "for finding in executive_summary['key_findings']:\n",
    "    print(f\"   {finding}\")\n",
    "\n",
    "print(\"\\nCRITICAL AREAS REQUIRING IMMEDIATE ATTENTION:\")\n",
    "for area in executive_summary['critical_areas']:\n",
    "    ej_flag = \" [EJ Community]\" if area['in_ej_community'] else \"\"\n",
    "    print(f\"   Station {area['station_id']}: WSLUS={area['wslus_score']:.1f}, \"\n",
    "          f\"Primary issue: {area['primary_issue']}{ej_flag}\")\n",
    "\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "for i, rec in enumerate(executive_summary['recommendations'], 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "if executive_summary['impact_estimates']:\n",
    "    print(\"\\nESTIMATED IMPACT OF INTERVENTIONS:\")\n",
    "    for key, value in executive_summary['impact_estimates'].items():\n",
    "        print(f\"   {key.replace('_', ' ').title()}: {value:.1f if isinstance(value, float) else value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results and Next Steps\n",
    "\n",
    "### 10.1 Save Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "stations_clustered.to_csv('stations_with_wslus_scores.csv', index=False)\n",
    "print(\" Saved station analysis to 'stations_with_wslus_scores.csv'\")\n",
    "\n",
    "if len(preservation_opportunities) > 0:\n",
    "    preservation_opportunities.to_csv('preservation_opportunities.csv', index=False)\n",
    "    print(\" Saved preservation opportunities to 'preservation_opportunities.csv'\")\n",
    "\n",
    "# Save model\n",
    "if model is not None:\n",
    "    import joblib\n",
    "    joblib.dump(model, 'wslus_predictive_model.pkl')\n",
    "    print(\" Saved predictive model to 'wslus_predictive_model.pkl'\")\n",
    "\n",
    "# Save summary as JSON\n",
    "with open('executive_summary.json', 'w') as f:\n",
    "    json.dump(executive_summary, f, indent=2, default=str)\n",
    "print(\" Saved executive summary to 'executive_summary.json'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGenerated outputs:\")\n",
    "print(\"  1. watershed_preservation_map.html - Interactive map\")\n",
    "print(\"  2. watershed_analysis_dashboard.html - Analysis dashboard\")\n",
    "print(\"  3. stations_with_wslus_scores.csv - Complete station analysis\")\n",
    "print(\"  4. preservation_opportunities.csv - Ranked preservation targets\")\n",
    "print(\"  5. wslus_predictive_model.pkl - ML model for predictions\")\n",
    "print(\"  6. executive_summary.json - Key findings and recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Next Steps and Deployment Recommendations\n",
    "\n",
    "## Next Steps for Implementation:\n",
    "\n",
    "### 1. **Data Integration**\n",
    "   - Connect to live MassGIS data feeds\n",
    "   - Integrate real-time water quality monitoring data\n",
    "   - Add precipitation and storm event data for temporal analysis\n",
    "\n",
    "### 2. **Model Refinement**\n",
    "   - Calibrate thresholds using Massachusetts-specific water quality standards\n",
    "   - Incorporate seasonal variations and storm response patterns\n",
    "   - Add economic valuation of preservation benefits\n",
    "\n",
    "### 3. **Web Application Development**\n",
    "   - Build interactive dashboard using Dash or Streamlit\n",
    "   - Create API endpoints for real-time WSLUS calculations\n",
    "   - Implement user authentication for different stakeholder groups\n",
    "\n",
    "### 4. **Stakeholder Engagement**\n",
    "   - Present to MassDEP, conservation organizations, and EJ communities\n",
    "   - Gather feedback and refine scoring methodology\n",
    "   - Develop training materials and documentation\n",
    "\n",
    "### 5. **Policy Integration**\n",
    "   - Align with Massachusetts watershed management plans\n",
    "   - Support grant applications with quantitative preservation metrics\n",
    "   - Create regular reporting mechanisms for tracking progress\n",
    "\n",
    "### 6. **Expansion Opportunities**\n",
    "   - Scale to cover entire Massachusetts watershed network\n",
    "   - Integrate climate change projections\n",
    "   - Add cost-benefit analysis for specific interventions\n",
    "   - Develop mobile app for field data collection\n",
    "\n",
    "## Technical Deployment Architecture:\n",
    "\n",
    "```\n",
    "          \n",
    " Water        ETL Pipeline  PostgreSQL/   \n",
    " Quality           (Airflow)          PostGIS DB    \n",
    " Sensors               \n",
    "                                  \n",
    "                                                \n",
    "                                  \n",
    " MassGIS          \n",
    " Data Feeds        WSLUS Model   Web Dashboard \n",
    "      (Python API)       (React/Dash)  \n",
    "                        \n",
    "                                                \n",
    "                                  \n",
    " Weather          \n",
    " Data              ML Pipeline   Mobile App    \n",
    "      (MLflow)           (React Native)\n",
    "                        \n",
    "```\n",
    "\n",
    "This notebook provides the foundation for a powerful decision support system that bridges the gap between water quality monitoring and land conservation priorities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
